新文件：batch_engine.py

文件开始
import time

import numpy as np
import torch
from torch.nn.utils import clip_grad_norm_
from tqdm import tqdm

from tools.utils import AverageMeter, to_scalar, time_str

img_count=0

def batch_trainer(epoch, model, ViT_model, train_loader, criterion, optimizer):
    global img_count
    model.train()
    ViT_model.train()
    epoch_time = time.time()
    loss_meter = AverageMeter()
    batch_num = len(train_loader)
    gt_list = []
    preds_probs = []
    save_name=[]
    lr = optimizer.param_groups[0]['lr']
    print(f'learning rate whith VTB:{lr}')

    for step, (imgs, gt_label, imgname) in enumerate(train_loader):
        for elem in imgname :
            save_name.append(elem)
        img_count+=imgs.shape[0]
        batch_time = time.time()
        imgs, gt_label = imgs.cuda(), gt_label.cuda()

        train_logits = model(imgs,ViT_model=ViT_model)
        train_loss = criterion(train_logits, gt_label)
        optimizer.zero_grad()

        train_loss.backward()
        optimizer.step()
        loss_meter.update(to_scalar(train_loss))

        gt_list.append(gt_label.cpu().numpy())
        train_probs = torch.sigmoid(train_logits)
        preds_probs.append(train_probs.detach().cpu().numpy())

        log_interval = 500
        if (step + 1) % log_interval == 0 or (step + 1) % len(train_loader) == 0:
            print(f'{time_str()}, Step {step}/{batch_num} in Ep {epoch}, {(time.time() - batch_time)/imgs.shape[0]:.4f}s ',
                  f'train_loss:{loss_meter.val:.4f}')
    train_loss = loss_meter.avg

    gt_label = np.concatenate(gt_list, axis=0)
    preds_probs = np.concatenate(preds_probs, axis=0)

    print(f'Epoch {epoch}, LR {lr}, Train_Time {time.time() - epoch_time:.2f}s, Loss: {loss_meter.avg:.4f},img_num:{img_count}')
    img_count=0
    return train_loss, gt_label, preds_probs


def valid_trainer(epoch,model,ViT_model, valid_loader, criterion):
    model.eval()
    ViT_model.eval()
    loss_meter = AverageMeter()
    batch_num = len(valid_loader)
    preds_probs = []
    gt_list = []
    save_name=[]
    save_dir=[]
    with torch.no_grad():
        for step, (imgs, gt_label, imgname) in enumerate(valid_loader):
            imgs = imgs.cuda()
            gt_label = gt_label.cuda()
            gt_list.append(gt_label.cpu().numpy())
            valid_logits = model(imgs,ViT_model=ViT_model )
            
            valid_loss = criterion(valid_logits, gt_label)
            valid_probs = torch.sigmoid(valid_logits)
            preds_probs.append(valid_probs.cpu().numpy())
            loss_meter.update(to_scalar(valid_loss))
    valid_loss = loss_meter.avg

    gt_label = np.concatenate(gt_list, axis=0)
    preds_probs = np.concatenate(preds_probs, axis=0)
    return valid_loss, gt_label, preds_probs

文件结束
新文件：config.py

文件开始
import argparse


def argument_parser():
    parser = argparse.ArgumentParser(description="attribute recognition",
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument("dataset", type=str, default="MARS")
    
    parser.add_argument("--batchsize", type=int, default=16)
    parser.add_argument("--epoch", type=int, default=30)
    parser.add_argument("--height", type=int, default=224)
    parser.add_argument("--width", type=int, default=224)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    
    parser.add_argument("--length", type=int, default=15)
    parser.add_argument("--frames", type=int, default=6)
    
    parser.add_argument("--train_split", type=str, default="trainval", choices=['train', 'trainval'])
    parser.add_argument("--valid_split", type=str, default="test", choices=['test', 'valid'])
    
    parser.add_argument('--gpus', default='1', type=str, help='gpu device ids for CUDA_VISIBLE_DEVICES')
    parser.add_argument("--redirector", action='store_false')
    parser.add_argument("--epoch_save_ckpt", type=int, default=100)

    parser.add_argument("--check_point", type=bool, default=False)
    parser.add_argument("--dir", type=str, default=None)
    parser.add_argument("--avg_frame_extract", action='store_true')
    parser.add_argument("--without_temporal", action='store_true')
    parser.add_argument("--without_spatial", action='store_true')
    parser.add_argument("--clip_model", type=str, default="ViT-B/16", choices=['ViT-B/16', 'ViT-L/14'])
    parser.add_argument("--fusion_type", type=str, default='add')
    parser.add_argument("--spatial_interact_map", type=str, default='default',choices=['default', '0', '1', '2', '3', '4', '5', '6', '7'])
    parser.add_argument("--temporal_interact_map", type=str, default='default',choices=['default', '0', '1', '2', '3', '4', '5', '6', '7'])
    parser.add_argument("--spatial_feat_aggregation", type=str, default='mean_pooling',choices=['mean_pooling', 'LSTM', 'GRU', 'MLP'])
    parser.add_argument("--temporal_feat_aggregation", type=str, default='mean_pooling',choices=['mean_pooling', 'LSTM', 'GRU', 'MLP'])
    parser.add_argument("--mmformer_dim", type=int, default=240)
    parser.add_argument("--spatial_dim", type=int, default=240, choices=[144,240,256,336,432,512,768])
    parser.add_argument("--temporal_dim", type=int, default=240, choices=[144,240,256,336,432,512,768])
    return parser


文件结束
新文件：README.md

文件开始

# VTFPAR++: Spatio-Temporal Side Tuning Pre-trained Foundation Models for Video-based Pedestrian Attribute Recognition
> **Spatio-Temporal Side Tuning Pre-trained Foundation Models for Video-based Pedestrian Attribute Recognition**, Xiao Wang, Qian Zhu, Jiandong Jin, Jun Zhu, Futian Wang*, Bo Jiang*, Yaowei Wang, Yonghong Tian
[[arXiv](https://arxiv.org/abs/2404.17929)]
 
## Abstract 
Existing pedestrian attribute recognition (PAR) algorithms are mainly developed based on a static image, however, the performance is unreliable in challenging scenarios, such as heavy occlusion, motion blur, etc. In this work, we propose to understand human attributes using video frames that can fully use temporal information by fine-tuning a pre-trained multi-modal foundation model efficiently. Specifically, we formulate the video-based PAR as a vision-language fusion problem and adopt a pre-trained foundation model CLIP to extract the visual features. More importantly, we propose a novel spatiotemporal side-tuning strategy to achieve parameter-efficient optimization of the pre-trained vision foundation model. To better utilize the semantic information, we take the full attribute list that needs to be recognized as another input and transform the attribute words/phrases into the corresponding sentence via split, expand, and prompt operations. Then, the text encoder of CLIP is utilized for embedding processed attribute descriptions. The averaged visual tokens and text tokens are concatenated and fed into a fusion Transformer for multi-modal interactive learning. The enhanced tokens will be fed into a classification head for pedestrian attribute prediction. Extensive experiments on two large-scale video-based PAR datasets fully validated the effectiveness of our proposed framework.

## Requirements
we use single RTX3090 24G GPU for training and evaluation.

**Basic Environment**
```
Python 3.9.16
pytorch 1.12.1
torchvision 0.13.1
```
**Installation**
```
pip install -r requirements.txt
```




## Datasets and Checkpoint 

* **MARS Dataset**:
```
链接：https://pan.baidu.com/s/16Krv3AAlBhB9JPa1EKDbLw 提取码：zi08
```

* **Checkpoint**
```
链接：https://pan.baidu.com/s/11ixJO0zPm1bXbBVudRYwAA 提取码：zzzz
```


**[Note]:**  If you can't access the Baidu drive, please check the **DropBox** 
```
https://www.dropbox.com/scl/fo/twpt7gu3gw7ssxfhqy4cl/AN_Jv68Iq3XsnLZk9Gdxf28?rlkey=bizjoyww3jdidf3geergd2l9b&st=p2h4b963&dl=0
```



## Training and Testing 
Use the following code to learn a model for MARS Dataset:

Training
```
python ./dataset/preprocess/mars.py
python train.py MARS
```
Testing
```
python eval.py MARS
```



## Demo Video 
A demo video can be found by clicking the image below: [[Youtube](https://youtu.be/yaeLMrr8MxU?si=aMmJk3epJJ76rnrL)] 

<p align="center">
<a href="[https://youtu.be/U4uUjci9Gjc](https://youtu.be/yaeLMrr8MxU?si=ZFha5XZsIG4g8E56)">
<img src="https://github.com/Event-AHU/OpenPAR/blob/main/VTFPAR%2B%2B/figures/VTFPAR%2B%2Bdemo.mp4_20240607_113607.505.jpg" alt="VTFPAR++_DemoVideo" width="700"/>
</a>
</p>






If you have any questions about this work, please submit an issue or contact me via **Email**: wangxiaocvpr@foxmail.com or xiaowang@ahu.edu.cn. Thanks for your attention! 

文件结束
新文件：requirements.txt

文件开始
scipy
torch
torchvision
tqdm
easydict
numpy
Pillow
sentence_transformers
tensorboardX
ftfy
regex
文件结束
新文件：train.py

文件开始
import os
import pprint
from collections import OrderedDict, defaultdict
import sys
import numpy as np
import torch
from torch.utils.data import DataLoader
import time
from torch import nn,optim
torch.autograd.set_detect_anomaly(True)
from batch_engine import valid_trainer, batch_trainer
from config import argument_parser
from dataset.AttrDataset import MultiModalAttrDataset, get_transform
from loss.CE_loss import *
from models.base_block import *
from models.sidenet import *
from tools.function import get_pedestrian_metrics,get_signle_metrics, simple_par_metrics
from tools.utils import time_str, save_ckpt, ReDirectSTD, set_seed, select_gpus
from solver import make_optimizer
from solver.scheduler_factory import create_scheduler,make_scheduler

from CLIP.clip import clip
from CLIP.clip.model import *
from tensorboardX import SummaryWriter
set_seed(12240321)
device = "cuda" if torch.cuda.is_available() else "cpu"

ViT_model, ViT_preprocess = clip.load("ViT-B/16", device=device,download_root='/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian') 

def main(args):
    start_time=time_str()
    print(f'start_time is {start_time}')
    log_dir = os.path.join('logs', args.dataset)
    if not os.path.exists(log_dir):
        os.mkdir(log_dir)
    log_dir = os.path.join(log_dir, start_time)
    tb_writer = SummaryWriter(r'/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/Cross_ViT/exp')
    if not os.path.exists(log_dir):
        os.mkdir(log_dir)
    stdout_file = os.path.join(log_dir, f'stdout_{time_str()}.txt')
    if args.redirector:
        print('redirector stdout')
        ReDirectSTD(stdout_file, 'stdout', False)
    pprint.pprint(OrderedDict(args.__dict__))

    print('-' * 60)

    select_gpus(args.gpus)

    print(f'train set: {args.dataset} {args.train_split}, test set: {args.valid_split}')

    train_tsfm, valid_tsfm = get_transform(args) 

    train_set = MultiModalAttrDataset(args=args, split=args.train_split , transform=train_tsfm) 
    train_loader = DataLoader(
        dataset=train_set, 
        batch_size=args.batchsize, 
        shuffle=True,
        num_workers=8,
        pin_memory=True, 
    )
    
    valid_set = MultiModalAttrDataset(args=args, split=args.valid_split , transform=valid_tsfm) 
    valid_loader = DataLoader(
        dataset=valid_set,
        batch_size=args.batchsize,
        shuffle=False,
        num_workers=8,
        pin_memory=True,
    )

    labels = train_set.label
    sample_weight = labels.mean(0) 
    model = TransformerClassifier(ViT_model, train_set.attr_num, attr_words=train_set.attributes)
    
    if torch.cuda.is_available():
        model = model.cuda()
    for name, param in model.named_parameters():
        if 'clip_visual_extractor' in name:
            param.requires_grad = False
    
    criterion = CEL_Sigmoid(sample_weight, attr_idx=train_set.attr_num)
    lr = args.lr
    epoch_num = args.epoch
    start_epoch=1
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = create_scheduler(optimizer, num_epochs=epoch_num, lr=lr, warmup_t=5)
    count_parameters(model, ViT_model.transformer)

    best_metric, epoch = trainer(args=args,
                                 epoch=epoch_num,
                                 model=model,
                                 ViT_model=ViT_model,
                                 train_loader=train_loader,
                                 valid_loader=valid_loader,
                                 criterion=criterion,
                                 optimizer=optimizer,
                                 scheduler=scheduler,
                                 path=log_dir,
                                 tb_writer=tb_writer,
                                 start_epoch=start_epoch)
    
def trainer(args,epoch, model,ViT_model, train_loader, valid_loader, criterion, optimizer, scheduler,path,tb_writer,start_epoch):
    max_ma,max_acc,max_f1,=0,0,0
    start=time.time()
    valid_name_buf = []
    for i in range(start_epoch, epoch+1):
        scheduler.step(i)
        train_loss, train_gt, train_probs = batch_trainer(
            epoch=i,
            model=model,
            ViT_model=ViT_model,
            train_loader=train_loader,
            criterion=criterion,
            optimizer=optimizer
        )
        valid_loss, valid_gt, valid_probs = valid_trainer(
            epoch=epoch,
            model=model,
            ViT_model=ViT_model,
            valid_loader=valid_loader,
            criterion=criterion,
        )
        
        if args.dataset =='MARS' : 
            #MARS
            index_list=[0,1,2,3,4,5,6,7,8,9,15,20,29,39,43]
            group="top length, bottom type, shoulder bag, backpack, hat, hand bag, hair, gender, bottom length, pose, motion, top color, bottom color, age"
        else:
            #DUKE
            index_list=[0,1,2,3,4,5,6,7,8,14,19,28,36]
            group="backpack, shoulder bag, hand bag, boots, gender, hat, shoes, top length, pose, motion, top color, bottom color"
        group_f1=[]
        group_acc=[]
        group_prec=[]
        group_recall=[]
        for idx in range(len(index_list)-1):
            if index_list[idx+1]-index_list[idx] >1 :
                result=simple_par_metrics(valid_gt[:,index_list[idx]:index_list[idx+1]], valid_probs[:,index_list[idx]:index_list[idx+1]])
            elif idx < 9  :
                result=simple_par_metrics(valid_gt[:,index_list[idx]], valid_probs[:,index_list[idx]],signle=True)
            group_f1.append(result.f1) 
            group_acc.append(result.acc)  
            group_prec.append(result.prec)
            group_recall.append(result.recall)   
        average_instance_f1 = np.mean(group_f1)

        average_acc = np.mean(group_acc)
        average_prec = np.mean(group_prec)    
        average_recall = np.mean(group_recall)

        print(f'{time_str()}Evaluation on test set, valid_loss:{valid_loss:.4f}\n',
              f"Acc :{group} \n",','.join(str(elem)[:6] for elem in group_acc),'\n',
              f"Prec :",','.join(str(elem)[:6] for elem in group_prec),'\n',
              f"Recall :",','.join(str(elem)[:6] for elem in group_recall),'\n',
              f"F1 :{group}  \n",','.join(str(elem)[:6] for elem in group_f1),'\n',
              'average_acc: {:.4f},average_prec: {:.4f},average_recall: {:.4f},average_f1: {:.4f}'.format(average_acc, average_prec, average_recall, average_instance_f1)                 
                )
        print('-' * 60)        

        tb_writer.add_scalar("lr", optimizer.param_groups[0]['lr'], i )  
        #tb_writer.add_scalar("train_loss", train_loss, i )  
        tb_writer.add_scalar("valid_loss", valid_loss, i )  
        tb_writer.add_scalar("valid_F1", average_instance_f1, i )
        if i % args.epoch_save_ckpt == 0:
            torch.save({
                        'epoch': i,
                        'model_state_dict': model.state_dict(),
                        }, os.path.join(path, f"epoch{i}.pth"))              
        
def count_parameters(model, ViT_model):
    total_params = sum(p.numel() for p in model.parameters())
    total_params += sum(p.numel() for p in ViT_model.parameters())
    selected_params1 = []
    selected_params2 = []
    for name, param in model.named_parameters():
        if param.requires_grad == True:
            selected_params1.append(param)
      
    selected_params_count1 = sum(p.numel() for p in selected_params1)
    trainable_percentage = ((selected_params_count1) / total_params) * 100 if total_params > 0 else 0
    print(f"trainable params: {(selected_params_count1)} || all params: {total_params} || trainable%: {trainable_percentage:.12f}")
       
if __name__ == '__main__':
    parser = argument_parser()
    args = parser.parse_args()
    main(args)

   
文件结束
新文件：CLIP\clip\clip.py

文件开始
import hashlib
import os
import urllib
import warnings
from typing import Any, Union, List
from pkg_resources import packaging

import torch
from PIL import Image
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
from tqdm import tqdm

from .model import build_model
from .simple_tokenizer import SimpleTokenizer as _Tokenizer

try:
    from torchvision.transforms import InterpolationMode
    BICUBIC = InterpolationMode.BICUBIC
except ImportError:
    BICUBIC = Image.BICUBIC


if packaging.version.parse(torch.__version__) < packaging.version.parse("1.7.1"):
    warnings.warn("PyTorch version 1.7.1 or higher is recommended")


__all__ = ["available_models", "load", "tokenize"]
_tokenizer = _Tokenizer()

_MODELS = {
    "RN50": "https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt",
    "RN101": "https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt",
    "RN50x4": "https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt",
    "RN50x16": "https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt",
    "RN50x64": "https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt",
    "ViT-B/32": "https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt",
    "ViT-B/16": "https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt",
    "ViT-L/14": "https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt",
    "ViT-L/14@336px": "https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt",
}


def _download(url: str, root: str):
    os.makedirs(root, exist_ok=True)
    filename = os.path.basename(url)

    expected_sha256 = url.split("/")[-2]
    download_target = os.path.join(root, filename)

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f"{download_target} exists and is not a regular file")

    if os.path.isfile(download_target):
        if hashlib.sha256(open(download_target, "rb").read()).hexdigest() == expected_sha256:
            return download_target
        else:
            warnings.warn(f"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file")

    with urllib.request.urlopen(url) as source, open(download_target, "wb") as output:
        with tqdm(total=int(source.info().get("Content-Length")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                loop.update(len(buffer))

    if hashlib.sha256(open(download_target, "rb").read()).hexdigest() != expected_sha256:
        raise RuntimeError(f"Model has been downloaded but the SHA256 checksum does not not match")

    return download_target


def _convert_image_to_rgb(image):
    return image.convert("RGB")


def _transform(n_px):
    return Compose([
        Resize(n_px, interpolation=BICUBIC),
        CenterCrop(n_px),
        _convert_image_to_rgb,
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])


def available_models() -> List[str]:
    """Returns the names of available CLIP models"""
    return list(_MODELS.keys())


def load(name: str, device: Union[str, torch.device] = "cuda" if torch.cuda.is_available() else "cpu", jit: bool = False, download_root: str = None):
    """Load a CLIP model

    Parameters
    ----------
    name : str
        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict

    device : Union[str, torch.device]
        The device to put the loaded model

    jit : bool
        Whether to load the optimized JIT model or more hackable non-JIT model (default).

    download_root: str
        path to download the model files; by default, it uses "~/.cache/clip"

    Returns
    -------
    model : torch.nn.Module
        The CLIP model

    preprocess : Callable[[PIL.Image], torch.Tensor]
        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input
    """
    if name in _MODELS:
        model_path = _download(_MODELS[name], download_root or os.path.expanduser("~/.cache/clip"))
    elif os.path.isfile(name):
        model_path = name
    else:
        raise RuntimeError(f"Model {name} not found; available models = {available_models()}")

    with open(model_path, 'rb') as opened_file:
        try:
            # loading JIT archive
            model = torch.jit.load(opened_file, map_location=device if jit else "cpu").eval()
            state_dict = None
        except RuntimeError:
            # loading saved state dict
            if jit:
                warnings.warn(f"File {model_path} is not a JIT archive. Loading as a state dict instead")
                jit = False
            state_dict = torch.load(opened_file, map_location="cpu")

    if not jit:
        model = build_model(state_dict or model.state_dict()).to(device)
        if str(device) == "cpu":
            model.float()
        return model, _transform(model.visual.input_resolution)

    # patch the device names
    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])
    device_node = [n for n in device_holder.graph.findAllNodes("prim::Constant") if "Device" in repr(n)][-1]

    def patch_device(module):
        try:
            graphs = [module.graph] if hasattr(module, "graph") else []
        except RuntimeError:
            graphs = []

        if hasattr(module, "forward1"):
            graphs.append(module.forward1.graph)

        for graph in graphs:
            for node in graph.findAllNodes("prim::Constant"):
                if "value" in node.attributeNames() and str(node["value"]).startswith("cuda"):
                    node.copyAttributes(device_node)

    model.apply(patch_device)
    patch_device(model.encode_image)
    patch_device(model.encode_text)

    # patch dtype to float32 on CPU
    if str(device) == "cpu":
        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])
        float_input = list(float_holder.graph.findNode("aten::to").inputs())[1]
        float_node = float_input.node()

        def patch_float(module):
            try:
                graphs = [module.graph] if hasattr(module, "graph") else []
            except RuntimeError:
                graphs = []

            if hasattr(module, "forward1"):
                graphs.append(module.forward1.graph)

            for graph in graphs:
                for node in graph.findAllNodes("aten::to"):
                    inputs = list(node.inputs())
                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()
                        if inputs[i].node()["value"] == 5:
                            inputs[i].node().copyAttributes(float_node)

        model.apply(patch_float)
        patch_float(model.encode_image)
        patch_float(model.encode_text)

        model.float()

    return model, _transform(model.input_resolution.item())


def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:
    """
    Returns the tokenized representation of given input string(s)

    Parameters
    ----------
    texts : Union[str, List[str]]
        An input string or a list of input strings to tokenize

    context_length : int
        The context length to use; all CLIP models use 77 as the context length

    truncate: bool
        Whether to truncate the text in case its encoding is longer than the context length

    Returns
    -------
    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].
    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.
    """
    if isinstance(texts, str):
        texts = [texts]

    sot_token = _tokenizer.encoder["<|startoftext|>"]
    eot_token = _tokenizer.encoder["<|endoftext|>"]
    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]
    if packaging.version.parse(torch.__version__) < packaging.version.parse("1.8.0"):
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)
    else:
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)

    for i, tokens in enumerate(all_tokens):
        if len(tokens) > context_length:
            if truncate:
                tokens = tokens[:context_length]
                tokens[-1] = eot_token
            else:
                raise RuntimeError(f"Input {texts[i]} is too long for context length {context_length}")
        result[i, :len(tokens)] = torch.tensor(tokens)

    return result

文件结束
新文件：CLIP\clip\model.py

文件开始
from collections import OrderedDict
from typing import Tuple, Union

import numpy as np
import torch
import torch.nn.functional as F
from torch import nn


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1):
        super().__init__()

        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1
        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu1 = nn.ReLU(inplace=True)

        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.relu2 = nn.ReLU(inplace=True)

        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()

        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu3 = nn.ReLU(inplace=True)

        self.downsample = None
        self.stride = stride

        if stride > 1 or inplanes != planes * Bottleneck.expansion:
            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1
            self.downsample = nn.Sequential(OrderedDict([
                ("-1", nn.AvgPool2d(stride)),
                ("0", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),
                ("1", nn.BatchNorm2d(planes * self.expansion))
            ]))

    def forward(self, x: torch.Tensor):
        identity = x

        out = self.relu1(self.bn1(self.conv1(x)))
        out = self.relu2(self.bn2(self.conv2(out)))
        out = self.avgpool(out)
        out = self.bn3(self.conv3(out))

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu3(out)
        return out


class AttentionPool2d(nn.Module):
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
        x, _ = F.multi_head_attention_forward(
            query=x, key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )

        return x[0]


class ModifiedResNet(nn.Module):
    """
    A ResNet class that is similar to torchvision's but contains the following changes:
    - There are now 3 "stem" convolutions as opposed to 1, with an average pool instead of a max pool.
    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1
    - The final pooling layer is a QKV attention instead of an average pool
    """

    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):
        super().__init__()
        self.output_dim = output_dim
        self.input_resolution = input_resolution

        # the 3-layer stem
        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(width // 2)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(width // 2)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(width)
        self.relu3 = nn.ReLU(inplace=True)
        self.avgpool = nn.AvgPool2d(2)

        # residual layers
        self._inplanes = width  # this is a *mutable* variable used during construction
        self.layer1 = self._make_layer(width, layers[0])
        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)
        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)
        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)

        embed_dim = width * 32  # the ResNet feature dimension
        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)

    def _make_layer(self, planes, blocks, stride=1):
        layers = [Bottleneck(self._inplanes, planes, stride)]

        self._inplanes = planes * Bottleneck.expansion
        for _ in range(1, blocks):
            layers.append(Bottleneck(self._inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        def stem(x):
            x = self.relu1(self.bn1(self.conv1(x)))
            x = self.relu2(self.bn2(self.conv2(x)))
            x = self.relu3(self.bn3(self.conv3(x)))
            x = self.avgpool(x)
            return x

        x = x.type(self.conv1.weight.dtype)
        x = stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.attnpool(x)

        return x


class LayerNorm(nn.LayerNorm):
    """Subclass torch's LayerNorm to handle fp16."""

    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        ret = super().forward(x.type(torch.float32))
        return ret.type(orig_type)


class QuickGELU(nn.Module):
    def forward(self, x: torch.Tensor):
        return x * torch.sigmoid(1.702 * x)


class ResidualAttentionBlock(nn.Module):
    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):
        super().__init__()

        self.attn = nn.MultiheadAttention(d_model, n_head)
        self.ln_1 = LayerNorm(d_model)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, d_model * 4)),
            ("gelu", QuickGELU()),
            ("c_proj", nn.Linear(d_model * 4, d_model))
        ]))
        self.ln_2 = LayerNorm(d_model)
        self.attn_mask = attn_mask

    def attention(self, x: torch.Tensor):
        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None
        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]

    def forward(self, x: torch.Tensor):
        x = x + self.attention(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x


class Transformer(nn.Module):
    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):
        super().__init__()
        self.width = width
        self.layers = layers
        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])

    def forward(self, x: torch.Tensor):
        return self.resblocks(x)


class VisionTransformer(nn.Module):
    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):
        super().__init__()
        self.input_resolution = input_resolution
        self.output_dim = output_dim
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)
        self.patch_size=patch_size
        scale = width ** -0.5
        self.class_embedding = nn.Parameter(scale * torch.randn(width))
        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))
        self.ln_pre = LayerNorm(width)

        self.transformer = Transformer(width, layers, heads)

        self.ln_post = LayerNorm(width)
        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))

    def forward(self, x: torch.Tensor):
        x = self.conv1(x)  # shape = [*, width, grid, grid]
        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]
        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]
        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]
        x = x + self.positional_embedding.to(x.dtype)
        x = self.ln_pre(x)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD

        x = self.ln_post(x[:, :, :])

        if self.proj is not None:
            x = x @ self.proj

        return x


class CLIP(nn.Module):
    def __init__(self,
                 embed_dim: int,
                 # vision
                 image_resolution: int,
                 vision_layers: Union[Tuple[int, int, int, int], int],
                 vision_width: int,
                 vision_patch_size: int,
                 # text
                 context_length: int,
                 vocab_size: int,
                 transformer_width: int,
                 transformer_heads: int,
                 transformer_layers: int
                 ):
        super().__init__()

        self.context_length = context_length

        if isinstance(vision_layers, (tuple, list)):
            vision_heads = vision_width * 32 // 64
            self.visual = ModifiedResNet(
                layers=vision_layers,
                output_dim=embed_dim,
                heads=vision_heads,
                input_resolution=image_resolution,
                width=vision_width
            )
        else:
            vision_heads = vision_width // 64
            self.visual = VisionTransformer(
                input_resolution=image_resolution,
                patch_size=vision_patch_size,
                width=vision_width,
                layers=vision_layers,
                heads=vision_heads,
                output_dim=embed_dim
            )

        self.transformer = Transformer(
            width=transformer_width,
            layers=transformer_layers,
            heads=transformer_heads,
            attn_mask=self.build_attention_mask()
        )

        self.vocab_size = vocab_size
        self.token_embedding = nn.Embedding(vocab_size, transformer_width)
        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))
        self.ln_final = LayerNorm(transformer_width)

        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

        self.initialize_parameters()

    def initialize_parameters(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.positional_embedding, std=0.01)

        if isinstance(self.visual, ModifiedResNet):
            if self.visual.attnpool is not None:
                std = self.visual.attnpool.c_proj.in_features ** -0.5
                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)

            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:
                for name, param in resnet_block.named_parameters():
                    if name.endswith("bn3.weight"):
                        nn.init.zeros_(param)

        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)
        attn_std = self.transformer.width ** -0.5
        fc_std = (2 * self.transformer.width) ** -0.5
        for block in self.transformer.resblocks:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)

        if self.text_projection is not None:
            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)

    def build_attention_mask(self):
        # lazily create causal attention mask, with full attention between the vision tokens
        # pytorch uses additive attention mask; fill with -inf
        mask = torch.empty(self.context_length, self.context_length)
        mask.fill_(float("-inf"))
        mask.triu_(1)  # zero out the lower diagonal
        return mask

    @property
    def dtype(self):
        return self.visual.conv1.weight.dtype

    def encode_image(self, image):
        return self.visual(image.type(self.dtype))

    def encode_text(self, text):
        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection

        return x

    def forward(self, image, text):
        image_features = self.encode_image(image)
        text_features = self.encode_text(text)

        # normalized features
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        text_features = text_features / text_features.norm(dim=1, keepdim=True)

        # cosine similarity as logits
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()

        # shape = [global_batch_size, global_batch_size]
        return logits_per_image, logits_per_text


def convert_weights(model: nn.Module):
    """Convert applicable model parameters to fp16"""

    def _convert_weights_to_fp16(l):
        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.half()
            if l.bias is not None:
                l.bias.data = l.bias.data.half()

        if isinstance(l, nn.MultiheadAttention):
            for attr in [*[f"{s}_proj_weight" for s in ["in", "q", "k", "v"]], "in_proj_bias", "bias_k", "bias_v"]:
                tensor = getattr(l, attr)
                if tensor is not None:
                    tensor.data = tensor.data.half()

        for name in ["text_projection", "proj"]:
            if hasattr(l, name):
                attr = getattr(l, name)
                if attr is not None:
                    attr.data = attr.data.half()

    model.apply(_convert_weights_to_fp16)


def build_model(state_dict: dict):
    vit = "visual.proj" in state_dict

    if vit:
        vision_width = state_dict["visual.conv1.weight"].shape[0]
        vision_layers = len([k for k in state_dict.keys() if k.startswith("visual.") and k.endswith(".attn.in_proj_weight")])
        vision_patch_size = state_dict["visual.conv1.weight"].shape[-1]
        grid_size = round((state_dict["visual.positional_embedding"].shape[0] - 1) ** 0.5)
        image_resolution = vision_patch_size * grid_size
    else:
        counts: list = [len(set(k.split(".")[2] for k in state_dict if k.startswith(f"visual.layer{b}"))) for b in [1, 2, 3, 4]]
        vision_layers = tuple(counts)
        vision_width = state_dict["visual.layer1.0.conv1.weight"].shape[0]
        output_width = round((state_dict["visual.attnpool.positional_embedding"].shape[0] - 1) ** 0.5)
        vision_patch_size = None
        assert output_width ** 2 + 1 == state_dict["visual.attnpool.positional_embedding"].shape[0]
        image_resolution = output_width * 32

    embed_dim = state_dict["text_projection"].shape[1]
    context_length = state_dict["positional_embedding"].shape[0]
    vocab_size = state_dict["token_embedding.weight"].shape[0]
    transformer_width = state_dict["ln_final.weight"].shape[0]
    transformer_heads = transformer_width // 64
    transformer_layers = len(set(k.split(".")[2] for k in state_dict if k.startswith(f"transformer.resblocks")))

    model = CLIP(
        embed_dim,
        image_resolution, vision_layers, vision_width, vision_patch_size,
        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers
    )

    for key in ["input_resolution", "context_length", "vocab_size"]:
        if key in state_dict:
            del state_dict[key]

    convert_weights(model)
    model.load_state_dict(state_dict)
    return model.eval()

文件结束
新文件：CLIP\clip\simple_tokenizer.py

文件开始
import gzip
import html
import os
from functools import lru_cache

import ftfy
import regex as re


@lru_cache()
def default_bpe():
    return os.path.join(os.path.dirname(os.path.abspath(__file__)), "bpe_simple_vocab_16e6.txt.gz")


@lru_cache()
def bytes_to_unicode():
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a signficant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    """
    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("¡"), ord("¬")+1))+list(range(ord("®"), ord("ÿ")+1))
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8+n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))


def get_pairs(word):
    """Return set of symbol pairs in a word.
    Word is represented as tuple of symbols (symbols being variable-length strings).
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs


def basic_clean(text):
    text = ftfy.fix_text(text)
    text = html.unescape(html.unescape(text))
    return text.strip()


def whitespace_clean(text):
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text


class SimpleTokenizer(object):
    def __init__(self, bpe_path: str = default_bpe()):
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        merges = gzip.open(bpe_path).read().decode("utf-8").split('\n')
        merges = merges[1:49152-256-2+1]
        merges = [tuple(merge.split()) for merge in merges]
        vocab = list(bytes_to_unicode().values())
        vocab = vocab + [v+'</w>' for v in vocab]
        for merge in merges:
            vocab.append(''.join(merge))
        vocab.extend(['<|startoftext|>', '<|endoftext|>'])
        self.encoder = dict(zip(vocab, range(len(vocab))))
        self.decoder = {v: k for k, v in self.encoder.items()}
        self.bpe_ranks = dict(zip(merges, range(len(merges))))
        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}
        self.pat = re.compile(r"""<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+""", re.IGNORECASE)

    def bpe(self, token):
        if token in self.cache:
            return self.cache[token]
        word = tuple(token[:-1]) + ( token[-1] + '</w>',)
        pairs = get_pairs(word)

        if not pairs:
            return token+'</w>'

        while True:
            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except:
                    new_word.extend(word[i:])
                    break

                if word[i] == first and i < len(word)-1 and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)
        word = ' '.join(word)
        self.cache[token] = word
        return word

    def encode(self, text):
        bpe_tokens = []
        text = whitespace_clean(basic_clean(text)).lower()
        for token in re.findall(self.pat, text):
            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))
            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
        return bpe_tokens

    def decode(self, tokens):
        text = ''.join([self.decoder[token] for token in tokens])
        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors="replace").replace('</w>', ' ')
        return text

文件结束
新文件：CLIP\clip\__init__.py

文件开始
from .clip import *

文件结束
新文件：dataset\AttrDataset.py

文件开始
import os
import pickle
from random import sample
import numpy as np
import torch
import torch.utils.data as data
from PIL import Image
from torch.utils.data import DataLoader
from config import argument_parser
from tools.function import get_pkl_rootpath
import torchvision.transforms as T
from operator import itemgetter
class MultiModalAttrDataset(data.Dataset):

    def __init__(self, split, args, transform=None, target_transform=None):
        assert args.dataset in ['MARS', 'DUKE'], \
            f'dataset name {args.dataset} is not exist,The legal name is MARS, DUKE'
        self.args = args

        if args.dataset =='MARS' :
            data_path='/path'
        else :
            data_path='/path'

        dataset_info = pickle.load(open(data_path, 'rb+'))

        track_id = dataset_info.track_name
        attr_label = dataset_info.label

        assert split in dataset_info.partition.keys(), f'split {split} is not exist'
        self.frames=args.frames
        self.dataset = args.dataset
        self.transform = transform
        self.target_transform = target_transform
        self.root_path = dataset_info.root
        self.attr_id = dataset_info.attr_name
        self.attr_num = len(self.attr_id)
        self.attributes=dataset_info.attr_name
        self.track_idx=dataset_info.partition[split]

        if isinstance(self.track_idx, list):
            self.track_idx = self.track_idx[0]
        self.track_num = self.track_idx.shape[0]
        self.track_id = [track_id[i] for i in self.track_idx]
        self.label = np.array(itemgetter(*self.track_id)(dataset_info.result_dict)) 
        self.label_all = self.label
        self.label_word = dataset_info.words
        self.label_vector=dataset_info.attr_vectors
        self.words = self.label_word.tolist()
        self.track_imgs_path=dataset_info.track_imgs_path
        self.result_dict=dataset_info.result_dict

    def __getitem__(self, index):
        trackname= self.track_id[index]
        gt_label = self.result_dict[trackname]
        gt_label=np.array(gt_label)
        track_img_path =self.track_imgs_path[trackname]
        imgs=[]
        if self.args.avg_frame_extract:
            my_sample = select_images(track_img_path, self.frames)
        else:
            if len(track_img_path)<=(self.frames-1):
                my_sample=np.random.choice(track_img_path, self.frames)
            else:
                my_sample=sample(track_img_path,self.frames)
        assert len(my_sample)==self.frames,print(len(my_sample))
        for i in my_sample:
            pil=Image.open(i)
            imgs.append(pil)
        imgs_trans=[]
        if self.transform is not None:
            for i in imgs:
                imgs_trans.append(self.transform(i))

        gt_label = gt_label.astype(np.float32)
        label_v=self.label_vector.astype(np.float32)

        if self.target_transform is not None:
            gt_label = self.transform(gt_label)

        return torch.stack(imgs_trans), gt_label,trackname,label_v
    def __len__(self):
        return len(self.track_id)

#间隔抽帧排序
import math
def select_images(track_img_path, N):
    # 对图片路径列表进行排序
    sorted_img_paths = sorted(track_img_path)
    # 计算每个间隔的步长
    if len(sorted_img_paths) <= N:
        step = math.ceil(len(sorted_img_paths) / N)
    else:
        step = 1
    # 如果路径列表长度小于N，使用最后一帧进行填充
    while len(sorted_img_paths) < N:
        sorted_img_paths.append(sorted_img_paths[-1])
    # 从排序后的列表中均匀选取N个图片
    selected_images = [sorted_img_paths[i] for i in range(0, len(sorted_img_paths), step)][:N]
    return selected_images

def get_transform(args):
    height = args.height
    width = args.width
    normalize = T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    train_transform = T.Compose([
        T.Resize((height, width)),
        T.Pad(10),
        T.RandomCrop((height, width)),
        T.RandomHorizontalFlip(),
        T.ToTensor(),
        normalize,
    ])

    valid_transform = T.Compose([
        T.Resize((height, width)),
        T.ToTensor(),
        normalize
    ])

    return train_transform, valid_transform

文件结束
新文件：dataset\__init__.py

文件开始

文件结束
新文件：dataset\preprocess\duke.py

文件开始
import os
import numpy as np
import random
import pickle
import csv
from easydict import EasyDict
from scipy.io import loadmat
import glob
from sentence_transformers import SentenceTransformer

np.random.seed(0)
random.seed(0)

attr_words = [
    'carring backpack',#0
    'carring shoulder bag',#1
    'carring hand bag',#2
    'shoes boots',#3
    'female',#4
    'heaed hat',#5
    'shoes',#6
    'top long',#7
    'pose frontal','pose lateral-frontal','pose lateral','pose lateral-back','pose back','pose varies',#pose [8:14]
    'motion walking','motion running','motion riding','motion staying','motion varies',#motion[14:19]
    'top color black','top color white','top color red','top color purple','top color gray','top color blue','top color green','top color brown','top color complex',#[19:28]
    'bottom color black','bottom color white','bottom color red','bottom color grey','bottom color blue','bottom color green','bottom color brown','bottom color complex'#[28:36]
]


def track_imgpath(track_path):  
    images_path = glob.glob(os.path.join(track_path, "*.jpg"))
    for count, i in enumerate(images_path):
        images_path[count] = r'/'.join(i.split('\\'))
    return images_path


def generate_imgs(path, track_name):  
    imgs_path = {}
    for i in track_name:
        tracklet_path = path + '/' + str(i) + '/'
        result = track_imgpath(tracklet_path)
        imgs_path[i] = result
    return imgs_path


def make_dir(path):
    if os.path.exists(path):
        pass
    else:
        os.mkdir(path)


def get_label_embeds(labels):
    model = SentenceTransformer('all-mpnet-base-v2')
    embeddings = model.encode(labels)
    return embeddings

def generate_label(filename):
    with open(filename, "r") as csvfile:
        csv_reader = csv.reader(csvfile)
        header = next(csv_reader)  
        result_dict = {}
        for row in csv_reader:
            row_buf = [int(i) for i in row[1:]]
            result_dict[str(row[0])] = np.array(row_buf)
    return result_dict


def generate_data_description(save_dir):
    dataset = EasyDict()
    dataset.description = 'duke'
    dataset.root = os.path.join(save_dir, 'pad_dataset')
    dataset.attr_name = attr_words
    dataset.words = np.array(attr_words)
    result_dict = generate_label("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/DUKE_pre/duke_annotation/new_encoded.csv")
    trainval_name = []
    test_name = []
    trainval_gt_list = []
    test_gt_list = []
    track_name = []
    track_gt_list = []

    track_name_file = open("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/DUKE_pre/duke_annotation/duke_track_name.txt", 'r', encoding='utf8').readlines()
    for name in track_name_file:
        curLine = name.strip('\n')
        track_name.append(curLine)

    trainval_name_file = open("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/DUKE_pre/duke_annotation/trainval_name.txt", 'r', encoding='utf8').readlines()
    for name in trainval_name_file:
        curLine = name.strip('\n')
        trainval_name.append(curLine)

    test_name_file = open("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/DUKE_pre/duke_annotation/test_name.txt", 'r', encoding='utf8').readlines()
    for name in test_name_file:
        curLine = name.strip('\n')
        test_name.append(curLine)

    for gt in track_name:
        curLine = name.strip('\n')
        track_gt_list.append(result_dict[curLine])

    for gt in trainval_name:
        curLine = name.strip('\n')
        trainval_gt_list.append(result_dict[curLine])

    for gt in test_name_file:
        curLine = name.strip('\n')
        test_gt_list.append(result_dict[curLine])
    
    dataset.test_name = test_name  # 4908
    dataset.trainval_name = trainval_name  # 11452
    dataset.track_name = dataset.trainval_name + dataset.test_name
    dataset.trainval_gt_list = trainval_gt_list
    dataset.test_gt_list = test_gt_list
    dataset.track_gt_list = track_gt_list
    dataset.result_dict = result_dict
    dataset.label = np.concatenate((np.array(trainval_gt_list), np.array(test_gt_list)), axis=0)
    assert dataset.label.shape == (2636+2196, 36), \
            f'dataset.label.shape {dataset.label.shape}'

    dataset.partition = EasyDict()
    dataset.attr_name = attr_words
    dataset.partition.test = np.arange(2196, 2196 + 2636)  # np.array(range(90000, 100000))
    dataset.partition.trainval = np.arange(0, 2196)  # np.array(range(90000))
    path1 = "/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/pad_duke_dataset"
    dataset.track_imgs_path = generate_imgs(path1, track_name)
    with open(os.path.join(save_dir, 'pad_duke.pkl'), 'wb+') as f:
        pickle.dump(dataset, f)

if __name__ == "__main__":
    save_dir = '/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian'
    generate_data_description(save_dir)

文件结束
新文件：dataset\preprocess\mars.py

文件开始
import os
import numpy as np
import random
import pickle
import csv
from easydict import EasyDict
from scipy.io import loadmat
import glob
from sentence_transformers import SentenceTransformer

np.random.seed(0)
random.seed(0)
attr_words = [
    'top short', #top length 0 
    'bottom skirt', #bottom length 1
    'shoulder bag','backpack',#shoulder bag #backpack 2 3
    'hat', 'hand bag', 'long hair', 'female',# hat/hand bag/hair/gender 4 5 6 7
    'bottom short', #bottom type 8
    'frontal', 'lateral-frontal', 'lateral', 'lateral-back', 'back', 'pose varies',#pose[9:15]
    'walking', 'running','riding', 'staying', 'motion varies',#motion[15:20]
    'top black', 'top purple', 'top green', 'top blue','top gray', 'top white', 'top yellow', 'top red', 'top complex',#top color [20 :29]
    'bottom white','bottom purple', 'bottom black', 'bottom green', 'bottom gray', 'bottom pink', 'bottom yellow','bottom blue', 'bottom brown', 'bottom complex',#bottom color[29:39]
    'young', 'teenager', 'adult', 'old'#age[39:43]
]

def generate_imgs(path,track_name):
    imgs_path={}
    for _,i in enumerate(track_name):
        tracklet_path=path+'/'+str(i)+'/'
        imagenames=os.listdir(tracklet_path)
        for idx in range(len(imagenames)):
            imagenames[idx]= os.path.join(tracklet_path,imagenames[idx])
        imgs_path[i]=imagenames
    return imgs_path

def make_dir(path):
    if os.path.exists(path):
        pass
    else:
        os.mkdir(path)

def generate_label(filename):
    with open(filename, "r") as csvfile:
        csv_reader = csv.reader(csvfile)
        header = next(csv_reader) 
        result_dict = {}
        for row in csv_reader:
            row_buf=[int(i) for i in row[1:]]
            result_dict[str(row[0])] = np.array(row_buf)
    return result_dict

def generate_data_description(save_dir):

    dataset = EasyDict()
    dataset.description = 'mars'
    dataset.root=os.path.join(save_dir,'pad_mars_dataset')
    dataset.attr_name=attr_words
    dataset.words=np.array(attr_words)
    result_dict=generate_label("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/new_encoded2.csv")
    trainval_name=[]
    test_name=[]
    trainval_gt_list=[]
    test_gt_list=[]
    track_name=[]
    track_gt_list=[]
    track_name_file=open("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/track_name.txt",'r',encoding='utf8').readlines()
    for name in track_name_file :
        curLine=name.strip('\n')
        track_name.append(curLine)
    trainval_name_file=open("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/base_train.txt",'r',encoding='utf8').readlines()
    for name in trainval_name_file :
        curLine=name.strip('\n')
        trainval_name.append(curLine)
    test_name_file=open("/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/base_test.txt",'r',encoding='utf8').readlines()
    for name in test_name_file :
        curLine=name.strip('\n')
        test_name.append(curLine)

    for gt in track_name:
        curLine=name.strip('\n')
        track_gt_list.append(result_dict[curLine])

    for gt in trainval_name:
        curLine=name.strip('\n')
        trainval_gt_list.append(result_dict[curLine])

    for gt in test_name_file:
        curLine=name.strip('\n')
        test_gt_list.append(result_dict[curLine])
    
    dataset.test_name=test_name#4908
    dataset.trainval_name=trainval_name#11452
    dataset.track_name=dataset.trainval_name+dataset.test_name

    for elem in dataset.track_name:
        if len(elem)>0 :
            pass
        else :
            print("error")
    dataset.trainval_gt_list=trainval_gt_list
    dataset.test_gt_list=test_gt_list
    dataset.track_gt_list=track_gt_list
    dataset.result_dict = result_dict
    dataset.label = np.concatenate((np.array(trainval_gt_list),np.array(test_gt_list)), axis=0)
    assert dataset.label.shape == (8298+8062, 43)

    dataset.partition = EasyDict()
    dataset.attr_name = attr_words
    dataset.partition.test = np.arange(8298, 8298+8062)  
    dataset.partition.trainval = np.arange(0, 8298)  
    
    path1="/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/pad_mars_dataset"
    dataset.track_imgs_path=generate_imgs(path1, track_name)

    with open(os.path.join(save_dir, 'mars_pad_right.pkl'), 'wb+') as f:
        pickle.dump(dataset, f)

if __name__ == "__main__":
    save_dir = '/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian'
    generate_data_description(save_dir)

文件结束
新文件：demo\gui_detection.py

文件开始
import cv2
import tkinter as tk
from tkinter import filedialog
from PIL import Image, ImageTk
import os
import numpy as np
from test import *
def make_square(img):
    # 获取图像的高度和宽度
    h, w, _ = img.shape

    # 计算填充后的图像的大小
    size = max(h, w)

    # 创建一个新的黑色背景图像
    square_img = np.zeros((size, size, 3), dtype=np.uint8)

    # 计算将图像放置在中心位置的坐标
    x_offset = (size - w) // 2
    y_offset = (size - h) // 2

    # 将图像放置在中心位置
    square_img[y_offset:y_offset + h, x_offset:x_offset + w, :] = img

    square_img=cv2.resize(square_img,(256,256))
    return square_img

def detect_people_yolo(frame, net, output_folder):
    height, width, channels = frame.shape

    # 将图像传递给模型进行前向传播
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    # 解析模型输出，提取行人位置
    class_ids = []
    confidences = []
    boxes = []
    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5 and class_id == 0:  # 0 corresponds to the class 'person'
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # 非最大值抑制，确保每个行人只被检测一次
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    # 截取帧图像并保存到专属文件夹
    for i in indices:
        box = boxes[i]
        x, y, w, h = box
        crop_img = frame[y:y+h, x:x+w]
        crop_img=make_square(crop_img)
        if crop_img.size!=0:
            # 获取行人的标识符
            person_id = f"person_{i}"

            # 检查该行人是否已经有专属文件夹，没有则创建
            # if person_id not in person_folders:
            #     person_folder = os.path.join(output_folder, person_id)
            #     os.makedirs(person_folder, exist_ok=True)
            #     person_folders[person_id] = person_folder

            # 保存截图到专属文件夹
            img_filename = f"snapshot_{len(os.listdir(output_folder)) + 1}.jpg"
            img_path = os.path.join(output_folder, img_filename)
            cv2.imwrite(img_path, crop_img)

def select_video():
    file_path = filedialog.askopenfilename(filetypes=[("MP4 files", "*.mp4")])
    video_path.set(file_path)
    cap = cv2.VideoCapture(file_path)
    _, frame = cap.read()
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    image = Image.fromarray(frame)
    photo = ImageTk.PhotoImage(image=image)
    video_label.config(image=photo)
    video_label.image = photo
    cap.release()

def start_detection():
    video_file = video_path.get()
    if not video_file:
        return

    cap = cv2.VideoCapture(video_file)
    while(cap.isOpened()):
        ret, frame = cap.read()
        if not ret:
            break

        detect_people_yolo(frame, net, output_folder)

        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image = Image.fromarray(frame)
        photo = ImageTk.PhotoImage(image=image)
        video_label.config(image=photo)
        video_label.image = photo
        window.update_idletasks()
        window.update()

    cap.release()
    #window.destroy()

attr_words = [
    'top short', #top length 0
    'bottom short', #bottom length 1
    'shoulder bag','backpack',#shoulder bag #backpack 2 3
    'hat', 'hand bag', 'long hair', 'female',# hat/hand bag/hair/gender 4 5 6 7
    'bottom skirt', #bottom type 8
    'frontal', 'lateral-frontal', 'lateral', 'lateral-back', 'back', 'pose varies',#pose[9:15]
    'walking', 'running','riding', 'staying', 'motion varies',#motion[15:20]
    'top black', 'top purple', 'top green', 'top blue','top gray', 'top white', 'top yellow', 'top red', 'top complex',#top color [20 :29]
    'bottom white','bottom purple', 'bottom black', 'bottom green', 'bottom gray', 'bottom pink', 'bottom yellow','bottom blue', 'bottom brown', 'bottom complex',#bottom color[29:39]
    'young', 'teenager', 'adult', 'old'#age[39:43]
]
index_list=[0,1,2,3,4,5,6,7,8,9,15,20,29,39,43]
group="top length, bottom length, shoulder bag, backpack, hat, hand bag, hair, gender, bottom type, pose, motion, top color, bottom color, age"

def update_text():
    result=main()
    text_widget.delete(1.0,tk.END)
    result=torch.sigmoid(result).squeeze()>0.45
    print(result)
    for i,item in enumerate(result):
        if item==True:
            print(item)
            if i in index_list:
                text_widget.insert(tk.END,f"{attr_words[i]},")

# 创建GUI窗口
window = tk.Tk()
window.title("Pedestrian Detection Demo")
window.geometry('640x320')

# 加载YOLO模型
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# 创建存储截图的主文件夹
output_folder = "person_snapshots"
os.makedirs(output_folder, exist_ok=True)

# 用于跟踪每个人的专属文件夹
person_folders = {}

# 创建GUI组件
video_path = tk.StringVar()
video_label = tk.Label(window)
video_label.pack()

video_button = tk.Button(window, text="Select Video", command=select_video)
video_button.config(height=2,width=20)
video_button.pack()

start_button = tk.Button(window, text="Start Detection", command=start_detection)
start_button.config(height=2,width=20)
start_button.pack()


text_widget = tk.Text(window, wrap=tk.WORD, width=60, height=5)
text_widget.pack(pady=10)

btn=tk.Button(window,text='生成结果',command=update_text)
btn.pack()


window.mainloop()

文件结束
新文件：demo\test.py

文件开始
import os
import cv2
from random import sample
import numpy as np
import torch
import torch.nn as nn
from PIL import Image
from torchvision import transforms
from CLIP.clip import clip
from models.vit import *
device= "cuda" if torch.cuda.is_available() else "cpu"

#预训练模型
ViT_model, ViT_preprocess = clip.load("ViT-B/16", device=device,download_root='C:/Users/Administrator/Desktop/video_detection')
ViT_model.eval()
#attr_names
attr_words = [
    'top short', #top length 0
    'bottom short', #bottom length 1
    'shoulder bag','backpack',#shoulder bag #backpack 2 3
    'hat', 'hand bag', 'long hair', 'female',# hat/hand bag/hair/gender 4 5 6 7
    'bottom skirt', #bottom type 8
    'frontal', 'lateral-frontal', 'lateral', 'lateral-back', 'back', 'pose varies',#pose[9:15]
    'walking', 'running','riding', 'staying', 'motion varies',#motion[15:20]
    'top black', 'top purple', 'top green', 'top blue','top gray', 'top white', 'top yellow', 'top red', 'top complex',#top color [20 :29]
    'bottom white','bottom purple', 'bottom black', 'bottom green', 'bottom gray', 'bottom pink', 'bottom yellow','bottom blue', 'bottom brown', 'bottom complex',#bottom color[29:39]
    'young', 'teenager', 'adult', 'old'#age[39:43]
]


checkpoint=torch.load('VTF-Pretrain.pth')
#print(checkpoint)


class TransformerClassifier(nn.Module):
    def __init__(self, attr_num, attr_words, dim=768, pretrain_path='C:/Users/Administrator/Desktop/video_detection/jx_vit_base_p16_224-80ecf9dd.pth'):
        super().__init__()
        super().__init__()
        self.attr_num = attr_num
        self.word_embed = nn.Linear(512, dim)
        self.visual_embed = nn.Linear(512, dim)
        self.vit = vit_base()
        self.vit.load_param(pretrain_path)
        self.blocks = self.vit.blocks[-1:]
        self.norm = self.vit.norm
        self.weight_layer = nn.ModuleList([nn.Linear(dim, 1) for i in range(self.attr_num)])
        self.bn = nn.BatchNorm1d(self.attr_num)
        self.text = clip.tokenize(attr_words).to(device)

    def forward(self, videos, ViT_model):
        ViT_features = []
        if len(videos.size()) < 5:
            videos=videos.unsqueeze(0)
        batch_size, num_frames, channels, height, width = videos.size()
        imgs = videos.view(-1, channels, height, width)
        # imgs=videos[:,0,:,:,:]
        # CLIP 提取视频帧特征
        for img in imgs:
            img = img.unsqueeze(0)
            ViT_features.append(ViT_model.encode_image(img).squeeze(0))
            # 图像特征
        ViT_image_features = torch.stack(ViT_features).to(device).float()

        _, token_num, visual_dim = ViT_image_features.size()
        ViT_image_features = ViT_image_features.view(batch_size, num_frames, token_num, visual_dim)

        ViT_image_features = self.visual_embed(torch.mean(ViT_image_features, dim=1))

        text_features = ViT_model.encode_text(self.text).to(device).float()
        textual_features = self.word_embed(text_features).expand(ViT_image_features.shape[0], text_features.shape[0],
                                                                 768)


        x = torch.cat([textual_features, ViT_image_features], dim=1)

        for b_c, blk in enumerate(self.blocks):
            x = blk(x)
        x = self.norm(x)
        logits = torch.cat([self.weight_layer[i](x[:, i, :]) for i in range(self.attr_num)], dim=1)

        #logits = self.bn(logits)
        return logits

#trainer



def main():
    model=TransformerClassifier(len(attr_words),attr_words=attr_words)
    if torch.cuda.is_available():
        model=model.cuda()
    checkpoint=torch.load('VTF-Pretrain.pth')
    model.load_state_dict(checkpoint['model_state_dict'],strict=False)
    start_epoch=1
    files = os.listdir('person_snapshots')

    trans = transforms.Compose([transforms.ToTensor(),
                                transforms.Resize(size=[224,224]),
                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    imgs=[]
    my_sample=sample(files,6)
    for i in my_sample:
        i=os.path.join('person_snapshots',i)
        pil=Image.open(i)
        pil=trans(pil)
        imgs.append(pil)
    imgs=torch.stack(imgs)
    imgs=imgs.to(device)
    valid_logits=model(imgs,ViT_model)
    return valid_logits

if __name__ == '__main__':
    result=main()
    result=result.squeeze()
    print(result)
    result=torch.sigmoid(result)
    print(result>0.45)

文件结束
新文件：demo\test.txt

文件开始


文件结束
新文件：loss\CE_loss.py

文件开始
from __future__ import absolute_import
from __future__ import division

import torch
import torch.nn as nn
import torch.nn.functional as F

from tools.function import ratio2weight


class CEL_Sigmoid(nn.Module):
    def __init__(self, sample_weight=None, size_average=True, attr_idx=None):
        super(CEL_Sigmoid, self).__init__()

        self.sample_weight = sample_weight
        self.size_average = size_average
        self.attr_idx = attr_idx

    def forward(self, logits, targets):
        batch_size = logits.shape[0]

        loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')

        targets_mask = torch.where(targets.detach().cpu() > 0.5, torch.ones(1), torch.zeros(1))
        if self.sample_weight is not None:
            if self.attr_idx is not None and targets_mask.shape[1] != self.sample_weight.shape[0]:
                weight = ratio2weight(targets_mask[:, self.attr_idx], self.sample_weight)
                loss = loss[:, self.attr_idx]
            else:
                weight = ratio2weight(targets_mask, self.sample_weight)
            # import pdb;pdb.set_trace()
            loss = (loss * weight.cuda())

        loss = loss.sum() / batch_size if self.size_average else loss.sum()

        return loss

文件结束
新文件：loss\__init__.py

文件开始

文件结束
新文件：models\attn_helper.py

文件开始
import warnings
from typing import Optional

import torch
from torch import Tensor
from torch.nn import functional as F
from open_clip.transformer import ResidualAttentionBlock


def cross_attn_with_self_bias(
    self, query, key, value, attn_mask=None, need_weights=False, key_padding_mask=None
):
    return cross_attn_with_self_bias_func(
        query,
        key,
        value,
        self.embed_dim,
        self.num_heads,
        self.in_proj_weight,
        self.in_proj_bias,
        self.bias_k,
        self.bias_v,
        self.add_zero_attn,
        self.dropout,
        self.out_proj.weight,
        self.out_proj.bias,
        training=self.training,
        key_padding_mask=key_padding_mask,
        need_weights=need_weights,
        attn_mask=attn_mask,
    )


def cross_attn_with_self_bias_func(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    embed_dim_to_check: int,
    num_heads: int,
    in_proj_weight: Tensor,
    in_proj_bias: Tensor,
    bias_k: Optional[Tensor],
    bias_v: Optional[Tensor],
    add_zero_attn: bool,
    dropout_p: float,
    out_proj_weight: Tensor,
    out_proj_bias: Tensor,
    training: bool = True,
    key_padding_mask: Optional[Tensor] = None,
    need_weights: bool = True,
    attn_mask: Optional[Tensor] = None,
    use_separate_proj_weight: bool = False,
    q_proj_weight: Optional[Tensor] = None,
    k_proj_weight: Optional[Tensor] = None,
    v_proj_weight: Optional[Tensor] = None,
    static_k: Optional[Tensor] = None,
    static_v: Optional[Tensor] = None,
):
    tgt_len, bsz, embed_dim = query.size()
    assert embed_dim == embed_dim_to_check
    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)

    head_dim = embed_dim // num_heads
    assert head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"
    scaling = float(head_dim) ** -0.5

    if not use_separate_proj_weight:
        if (query is key or torch.equal(query, key)) and (
            key is value or torch.equal(key, value)
        ):
            raise NotImplementedError("self-attention is not implemented")

        elif key is value or torch.equal(key, value):
            _b = in_proj_bias
            _start = 0
            _end = embed_dim
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            q = F.linear(query, _w, _b)

            if key is None:
                assert value is None
                k = None
                v = None
                q_k = None
                q_v = None
            else:
                _b = in_proj_bias
                _start = embed_dim
                _end = None
                _w = in_proj_weight[_start:, :]
                if _b is not None:
                    _b = _b[_start:]
                k, v = F.linear(key, _w, _b).chunk(2, dim=-1)
                q_k, q_v = F.linear(query, _w, _b).chunk(2, dim=-1)
        else:
            _b = in_proj_bias
            _start = 0
            _end = embed_dim
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            q = F.linear(query, _w, _b)
            _b = in_proj_bias
            _start = embed_dim
            _end = embed_dim * 2
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            k = F.linear(key, _w, _b)
            q_k = F.linear(query, _w, _b)
            _b = in_proj_bias
            _start = embed_dim * 2
            _end = None
            _w = in_proj_weight[_start:, :]
            if _b is not None:
                _b = _b[_start:]
            v = F.linear(value, _w, _b)
            q_v = F.linear(query, _w, _b)
    else:
        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)
        len1, len2 = q_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == query.size(-1)

        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)
        len1, len2 = k_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == key.size(-1)

        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)
        len1, len2 = v_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == value.size(-1)

        if in_proj_bias is not None:
            q = F.linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])
            k = F.linear(
                key, k_proj_weight_non_opt, in_proj_bias[embed_dim : (embed_dim * 2)]
            )
            v = F.linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2) :])
        else:
            q = F.linear(query, q_proj_weight_non_opt, in_proj_bias)
            k = F.linear(key, k_proj_weight_non_opt, in_proj_bias)
            v = F.linear(value, v_proj_weight_non_opt, in_proj_bias)
    q = q * scaling

    if attn_mask is not None:
        assert (
            attn_mask.dtype == torch.float32
            or attn_mask.dtype == torch.float64
            or attn_mask.dtype == torch.float16
            or attn_mask.dtype == torch.uint8
            or attn_mask.dtype == torch.bool
        ), "Only float, byte, and bool types are supported for attn_mask, not {}".format(
            attn_mask.dtype
        )
        if attn_mask.dtype == torch.uint8:
            warnings.warn(
                "Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
            )
            attn_mask = attn_mask.to(torch.bool)

        if attn_mask.dim() == 2:
            attn_mask = attn_mask.unsqueeze(0)
            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:
                raise RuntimeError("The size of the 2D attn_mask is not correct.")
        elif attn_mask.dim() == 3:
            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:
                raise RuntimeError("The size of the 3D attn_mask is not correct.")
        else:
            raise RuntimeError(
                "attn_mask's dimension {} is not supported".format(attn_mask.dim())
            )

    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:
        warnings.warn(
            "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
        )
        key_padding_mask = key_padding_mask.to(torch.bool)

    if bias_k is not None and bias_v is not None:
        if static_k is None and static_v is None:
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = F.pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = F.pad(key_padding_mask, (0, 1))
        else:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
    else:
        assert bias_k is None
        assert bias_v is None

    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
    if k is not None:
        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
        q_k = q_k.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
    if v is not None:
        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
        q_v = q_v.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)

    if static_k is not None:
        assert static_k.size(0) == bsz * num_heads
        assert static_k.size(2) == head_dim
        k = static_k

    if static_v is not None:
        assert static_v.size(0) == bsz * num_heads
        assert static_v.size(2) == head_dim
        v = static_v

    src_len = k.size(1)

    if key_padding_mask is not None:
        assert key_padding_mask.size(0) == bsz
        assert key_padding_mask.size(1) == src_len

    if add_zero_attn:
        src_len += 1
        k = torch.cat(
            [
                k,
                torch.zeros(
                    (k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device
                ),
            ],
            dim=1,
        )
        v = torch.cat(
            [
                v,
                torch.zeros(
                    (v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device
                ),
            ],
            dim=1,
        )
        if attn_mask is not None:
            attn_mask = F.pad(attn_mask, (0, 1))
        if key_padding_mask is not None:
            key_padding_mask = F.pad(key_padding_mask, (0, 1))

    attn_output_weights = torch.bmm(q, k.transpose(1, 2))
    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]

    if attn_mask is not None:
        if attn_mask.dtype == torch.bool:
            attn_output_weights.masked_fill_(attn_mask, float("-inf"))
        else:
            attn_output_weights += attn_mask

    if key_padding_mask is not None:
        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
        attn_output_weights = attn_output_weights.masked_fill(
            key_padding_mask.unsqueeze(1).unsqueeze(2),
            float("-inf"),
        )
        attn_output_weights = attn_output_weights.view(
            bsz * num_heads, tgt_len, src_len
        )
    self_weight = (q * q_k).sum(dim=-1, keepdim=True) 
    total_attn_output_weights = torch.cat([attn_output_weights, self_weight], dim=-1)
    total_attn_output_weights = F.softmax(total_attn_output_weights, dim=-1)
    total_attn_output_weights = F.dropout(
        total_attn_output_weights, p=dropout_p, training=training
    )
    attn_output_weights = total_attn_output_weights[
        :, :, :-1
    ] 
    self_weight = total_attn_output_weights[:, :, -1:]  

    attn_output = torch.bmm(
        attn_output_weights, v
    )  
    attn_output = (
        attn_output + self_weight * q_v
    )  
    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]
    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
    attn_output = F.linear(attn_output, out_proj_weight, out_proj_bias)

    if need_weights:
        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
        return attn_output, attn_output_weights  # .sum(dim=1) / num_heads
    else:
        return attn_output, None


def cross_attn_layer(self: ResidualAttentionBlock, x, mem, attn_bias):
    q_x = self.ln_1(x)
    k_x = v_x = self.ln_1(mem)
    x = x + self.ls_1(
        cross_attn_with_self_bias(self.attn, q_x, k_x, v_x, attn_mask=attn_bias)[0]
    )
    x = x + self.ls_2(self.mlp(self.ln_2(x)))
    return x


def downsample2d(src, target_shape, method="nearest"):
    if method in ["bicubic", "bilinear", "nearest"]:
        src = F.interpolate(src, size=target_shape, mode=method, align_corners=False)
    elif method == "avg":
        src = F.adaptive_avg_pool2d(src, output_size=target_shape)
    elif method == "max":
        src = F.adaptive_max_pool2d(src, output_size=target_shape)
    return src


def resize_pos_embed2d(
    posemb,
    src_shape,
    tgt_shape,
    num_prefix_tokens=1,
    interpolation="bicubic",
    antialias=False,
):
    """interpolate positional embedding from src_shape to tgt_shape. posemb: [N,L,C]"""
    if src_shape == tgt_shape:
        return posemb
    if num_prefix_tokens:
        posemb_prefix, posemb_grid = (
            posemb[:, :num_prefix_tokens],
            posemb[:, num_prefix_tokens:],
        )
    else:
        posemb_prefix, posemb_grid = posemb[:, :0], posemb

    posemb_grid = posemb_grid.permute(0, 2, 1).reshape(
        1, -1, src_shape[0], src_shape[1]
    )

    posemb_grid = F.interpolate(
        posemb_grid,
        size=tgt_shape,
        mode=interpolation,
        antialias=antialias,
        align_corners=False,
    )
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(
        1, tgt_shape[0] * tgt_shape[1], -1
    )
    posemb = torch.cat([posemb_prefix, posemb_grid], dim=1)
    return posemb

文件结束
新文件：models\base_block.py

文件开始
import torch.nn as nn
import torch
from CLIP.clip import clip
from models.vit import *
from models.visual import *
from models.sidenet import *
from models.sidenet_vit import *
from models.temporal_sidenet import *
from models.CrossFrameSidenet import *
from config import argument_parser

parser = argument_parser()
args = parser.parse_args()
device = "cuda" if torch.cuda.is_available() else "cpu"
model_register = {144: 'vit_w144n6d8_patch16', 240: 'vit_w240n6d8_patch16', 256: 'vit_w256n8d8_patch16',
                336: 'vit_w336n6d8_patch16', 432: 'vit_w432n6d8_patch16', 512: 'vit_w512n8d8_patch16',
                768: 'vit_w768n8d8_patch16'}

interact_dict={
'default':["0->0", "3->1", "6->2", "9->3","11->4"],
'0':["0->0", "1->1", "2->2", "3->3","4->4"], 
'1':["7->0", "8->1", "9->2", "10->3","11->4"],
'2':["0->0", "3->1", "6->2", "9->3","11->4"], 
'3':["3->1", "6->2", "9->3","11->4"],
'4':["6->2", "9->3","11->4"], 
'5':["9->3","11->4"], '6':["11->4"],
'7':["0->0", "1->1", "2->2", "4->3","6->4","8->5", "10->6", "11->7"]
}

class TransformerClassifier(nn.Module):
    def __init__(self, ViT_model, attr_num,attr_words, dim=args.mmformer_dim, spatial_dim=args.spatial_dim, 
    temporal_dim=args.temporal_dim, pretrain_path='/media/amax/836e911f-c5c3-4c4b-91f2-41bb8f3f5cb6/DATA/zhuqian/jx_vit_base_p16_224-80ecf9dd.pth'):
        super().__init__()
        super().__init__()

        self.attr_num = attr_num
        self.dim = dim
        self.using_tem = not args.without_temporal 
        self.using_spa = not args.without_spatial

        print('Using Temporal Side Net' if self.using_tem else 'Without Temporal Side Net')
        print('Using Spatial Side Net' if self.using_spa else 'Without Spatial Side Net')

        self.spatial_interact_map = interact_dict[args.spatial_interact_map]

        print('-' * 60)
        print(f"The interaction pattern of the spatial side net is {self.spatial_interact_map}")

        self.temporal_interact_map =interact_dict[args.temporal_interact_map]

        print(f"The interaction pattern of the temporal side net is {self.temporal_interact_map}")
        print('-' * 60)
        print(f"The dimension of the spatial/temporal side net is [{args.spatial_dim}] / [{args.temporal_dim}]")
        
        spa_model_name = model_register[args.spatial_dim]
        tem_model_name = model_register[args.temporal_dim]

        print(f"The model of the spatial/temporal side net is [{spa_model_name}] / [{tem_model_name}]")

        model_name = model_register[args.mmformer_dim]

        print(f"The model of the mmformer is {model_name}")

        self.clip_visual_extractor = FeatureExtractor(ViT_model.visual, last_layer_idx=-1, frozen_exclude=["positional_embedding"])
        if self.using_spa:
            self.spatial_sidenet = SideAdapterNetwork(spa_model_name, args.fusion_type, self.spatial_interact_map, [7, 8])

        if self.using_tem:
            self.crossframe_sidenet = CrossFrameSideNetwork(tem_model_name, args.fusion_type, self.temporal_interact_map, [7, 8])

        self.img_type = ViT_model.dtype
        self.word_embed = nn.Linear(512, dim)
        self.visual_embed = nn.Linear(768, dim)
        self.spa_embed = nn.Linear(spatial_dim, dim)
        self.tem_embed = nn.Linear(temporal_dim, dim)
        # mmformer
        vit = vit_base()
        vit.load_param(pretrain_path)
        
        vit_model= create_model(
            model_name,#vit_w240n6d8_patch16 vit_base_patch16_224
            False,#False
            img_size=224,#640-->224
            drop_path_rate=0.1,
            fc_norm=False,
            num_classes=0,
            embed_layer=PatchEmbed,
        )
        
        self.blocks = vit_model.blocks[-1:]
        self.norm = vit_model.norm
        self.weight_layer = nn.ModuleList([nn.Linear(dim, 1) for i in range(self.attr_num)])
        self.bn = nn.BatchNorm1d(self.attr_num)
        self.text = clip.tokenize(attr_words).to(device)
        self.w_temporal = nn.Parameter(torch.ones(1))
        input_size = 197 * dim
        print(f"The features aggregation of the spatial/temporal side net is [{args.spatial_feat_aggregation}] / [{args.temporal_feat_aggregation}]")

        if args.spatial_feat_aggregation == 'LSTM':
            self.spatial_feat_aggregation = nn.LSTM(input_size=dim, hidden_size=dim, num_layers=1, batch_first=True)
        elif args.spatial_feat_aggregation == 'GRU':
            self.spatial_feat_aggregation = nn.GRU(input_size=dim, hidden_size=dim, num_layers=1, batch_first=True)
        elif args.spatial_feat_aggregation =='MLP':
            self.spatial_feat_aggregation = nn.Conv1d(args.frames, 1, kernel_size=1)
        else:
            self.spatial_feat_aggregation = nn.AdaptiveAvgPool2d((1,240))

        if args.temporal_feat_aggregation == 'LSTM':
            self.temporal_feat_aggregation = nn.LSTM(input_size=dim, hidden_size=dim, num_layers=1, batch_first=True)
        elif args.temporal_feat_aggregation == 'GRU':
            self.temporal_feat_aggregation = nn.GRU(input_size=dim, hidden_size=dim, num_layers=1, batch_first=True)
        elif args.temporal_feat_aggregation =='MLP':
            self.temporal_feat_aggregation = nn.Conv1d(len(self.temporal_interact_map), 1, kernel_size=1)
        else:
            self.temporal_feat_aggregation = nn.AdaptiveAvgPool2d((1,240))


    def forward(self, videos,ViT_model):
        ViT_features=[]
        if len(videos.size())<5 :
            videos.unsqueeze(1) 

        batch_size, num_frames, channels, height, width = videos.size()
        imgs=videos.view(-1, channels, height, width)
        
        clip_image_features, _ = self.clip_visual_extractor(imgs.type(self.img_type))
        if self.using_spa:
            
            spatial_features = self.spatial_sidenet(imgs,clip_image_features)
            _,L,side_D = spatial_features.size()
            spatial_features = spatial_features.view(batch_size,num_frames,L,side_D)

            fuse_spatial_feat= torch.empty(batch_size, L, side_D).cuda()
            spatial_features = spatial_features.permute(0,2,1,3)
            for bb, batch_spatial_feat in enumerate(spatial_features):
                output = self.spatial_feat_aggregation(batch_spatial_feat)
                fuse_spatial_feat[bb] = output[0][:,-1,:] if isinstance(output, list) else output[:,-1,:]

        if self.using_tem:
            frame_select_feat={k:[] for k in range(num_frames)}
            for layer in self.crossframe_sidenet.fusion_map.values():
                selected_layer_feat = clip_image_features[layer]
                BF,D,H,W = selected_layer_feat.shape
                reshape_feat=selected_layer_feat.view(BF,D,-1).permute(0,2,1).view(batch_size,num_frames,H*W,D).permute(1,0,2,3)
                for fidx,frame_feat in enumerate(reshape_feat) :
                    frame_select_feat[fidx].append(frame_feat.cuda().float())
            temporal_features = self.crossframe_sidenet(frame_select_feat)
            num_select,B,L,side_D = temporal_features.size()
            temporal_features = temporal_features.permute(1,2,0,3)
            fuse_temporal_feat= torch.empty(batch_size, L , side_D).cuda()
            for bb, batch_temporal_feat in enumerate(temporal_features):
                output= self.temporal_feat_aggregation(batch_temporal_feat)
                fuse_temporal_feat[bb] = output[0][:,-1,:] if isinstance(output, list) else output[:,-1,:]
            
        if self.using_tem and self.using_spa:
            fusion_side_feat = fuse_spatial_feat + fuse_temporal_feat
        elif self.using_spa :
            fusion_side_feat = fuse_spatial_feat 
        elif self.using_tem :
            fusion_side_feat = fuse_temporal_feat

        text_features = ViT_model.encode_text(self.text).to(device).float()
        textual_features = self.word_embed(text_features).expand(batch_size, text_features.shape[0],self.dim)  
        x = torch.cat([textual_features, fusion_side_feat], dim=1)

        for b_c, blk in enumerate(self.blocks):
            x = blk(x)
        x = self.norm(x)
        logits = torch.cat([self.weight_layer[i](x[:, i, :]) for i in range(self.attr_num)], dim=1)
        logits = self.bn(logits)
        
        return logits
    

文件结束
新文件：models\CrossFrameSidenet.py

文件开始
import logging
from functools import partial
from typing import Dict, List, Tuple

import torch
from timm import create_model
from timm.models.vision_transformer import VisionTransformer
from torch import nn
from torch.nn import functional as F

from models.layers import MLP, build_fusion_layer
from models.timm_wrapper import PatchEmbed
from models.sidenet_vit import *

class CrossFrameSideNetwork(nn.Module):
    def __init__(self, model_name, fusion_type, fusion_map, depth=8):
        super().__init__()
        
        vit_model= create_model(
            model_name,#vit_w240n6d8_patch16 vit_base_patch16_224
            False,#False
            img_size=224,#640-->224
            drop_path_rate=0.0,#0.0
            fc_norm=False,
            num_classes=0,
            embed_layer=PatchEmbed,
        )

        if vit_model.cls_token is not None:
            vit_model.pos_embed = nn.Parameter(vit_model.pos_embed[:, 1:, ...])
        del vit_model.cls_token
        vit_model.cls_token = None

        del vit_model.norm
        vit_model.norm = nn.Identity()

        self.vit_model = vit_model
        self.num_features = vit_model.num_features
        self.cls_embed = nn.Parameter(torch.zeros(1, 1, self.num_features))
        self.cls_pos_embed = nn.Parameter(
            torch.zeros(1, 1, self.num_features)
        )
        nn.init.normal_(self.cls_embed, std=0.02)
        nn.init.normal_(self.cls_pos_embed, std=0.02)
        self.fusion_map = {int(j): int(i) for i, j in [x.split("->") for x in fusion_map]}
        fusion_type: str = 'cross1d'
        self.frist_linear=nn.Linear(768,self.num_features)
        fusion_layers = nn.ModuleDict(
            {
                f"layer_{tgt_idx}": build_fusion_layer(
                    fusion_type, 768, vit_model.num_features
                )   
                for tgt_idx in range(len(self.fusion_map.keys()))
            }
        )
        self.fusion_layers = fusion_layers
        
        self.projection_layers = nn.ModuleList([nn.Linear(self.num_features, self.num_features) for _ in range(len(self.fusion_map.keys()))])
        

    def forward(self, clip_features):
        x = clip_features[0]
        B,L,D=clip_features[0][0].shape
        pos_embed = self.vit_model.pos_embed 
        pos_embed = torch.cat(
            [self.cls_pos_embed.expand(pos_embed.shape[0], -1, -1), pos_embed], dim=1
        )

        x = torch.empty(len(clip_features[0]), B, L + 1, self.num_features).cuda()
        
        for frame_idx in clip_features.keys():
            feat_list = clip_features[frame_idx]
            blk = self.vit_model.blocks[frame_idx]
            if frame_idx == 0:
                for select_layer in range(len(feat_list)):
                    feat_list[select_layer] = torch.cat([self.cls_embed.expand(B, -1, -1), 
                                                        self.frist_linear(feat_list[select_layer].float())], dim=1) + pos_embed
                    feat_list[select_layer] = self.vit_model.norm_pre(feat_list[select_layer])
                    x[select_layer] = blk(feat_list[select_layer])
            else: 
                updated_x = x.clone()  
                for select_layer in range(len(feat_list)):
                    updated_x[select_layer] = blk(x[select_layer])
                    updated_x[select_layer] = self.fuse(select_layer, updated_x[select_layer], feat_list[select_layer])
                x = updated_x  
            
        return x

    def fuse(self,layer_idx,x,select_feat) :
        fusion_features = self.fusion_layers[f"layer_{layer_idx}"](x[:, 1:, ...], select_feat)
        x = torch.cat(
            [
                x[:, :1, ...],
                fusion_features,
            ],
            dim=1,
        )
        return x

文件结束
新文件：models\layers.py

文件开始
import fvcore.nn.weight_init as weight_init
import torch
from detectron2.layers import CNNBlockBase, Conv2d
from torch import nn
from torch.nn import functional as F


class LayerNorm(nn.Module):
    """
    A LayerNorm variant, popularized by Transformers, that performs point-wise mean and
    variance normalization over the channel dimension for inputs that have shape
    (batch_size, channels, height, width).
    https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa B950
    """

    def __init__(self, normalized_shape, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.normalized_shape = (normalized_shape,)

    def forward(self, x: torch.Tensor):
        u = x.mean(1, keepdim=True)
        s = (x - u).pow(2).mean(1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        x = self.weight[:, None, None] * x + self.bias[:, None, None]
        return x


class MLP(nn.Module):
    """Very simple multi-layer perceptron (also called FFN)"""

    def __init__(
        self, input_dim, hidden_dim, output_dim, num_layers, affine_func=nn.Linear
    ):
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(
            affine_func(n, k) for n, k in zip([input_dim] + h, h + [output_dim])
        )

    def forward(self, x: torch.Tensor):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x

class AddFusion(CNNBlockBase):
    def __init__(self, in_channels, out_channels):
        super().__init__(in_channels, out_channels, 1)
        self.input_proj = nn.Sequential(
            LayerNorm(in_channels),
            Conv2d(
                in_channels,
                out_channels,
                kernel_size=1,
            ),
        )
        weight_init.c2_xavier_fill(self.input_proj[-1])

    def forward(self, x: torch.Tensor, y: torch.Tensor, spatial_shape: tuple):
        y = (
            F.interpolate(
                self.input_proj(y.contiguous()),
                size=spatial_shape,
                mode="bilinear",
                align_corners=False,
            )
            .permute(0, 2, 3, 1)
            .reshape(x.shape)
        )
        x = x + y
        return x
    
class AddFusion1D(CNNBlockBase):
    def __init__(self, in_channels, out_channels):
        super().__init__(in_channels, out_channels, 1)
        self.ln = nn.LayerNorm(in_channels)
        self.input_proj =  torch.nn.Conv1d(
                in_channels,
                out_channels,
                kernel_size=1,
            )
        weight_init.c2_xavier_fill(self.input_proj)

    def forward(self, x: torch.Tensor, y: torch.Tensor):
        y = self.ln(y[:,1:])
        y = self.input_proj(y.contiguous().permute(0, 2, 1)).permute(0, 2, 1)
        x = x + y
        return x
    
class CrossFusion1D(CNNBlockBase):
    def __init__(self, in_channels, out_channels):
        super().__init__(in_channels, out_channels, 1)
        self.ln = nn.LayerNorm(in_channels)
        self.input_proj =  torch.nn.Conv1d(
                in_channels,
                out_channels,
                kernel_size=1,
            )
        weight_init.c2_xavier_fill(self.input_proj)

    def forward(self, x: torch.Tensor, y: torch.Tensor):
        y = self.ln(y)
        y = self.input_proj(y.contiguous().permute(0, 2, 1)).permute(0, 2, 1)
        x = x + y
        return x
    
def build_fusion_layer(fusion_type: str, in_channels: int, out_channels: int):
    if fusion_type == "add":
        return AddFusion(in_channels, out_channels)
    elif  fusion_type == "add1d":
        return AddFusion1D(in_channels, out_channels)
    elif fusion_type == "cross1d":
        return CrossFusion1D(in_channels, out_channels)
    else:
        raise ValueError("Unknown fusion type: {}".format(fusion_type))

文件结束
新文件：models\sidenet.py

文件开始
import logging
from functools import partial
from typing import Dict, List, Tuple

import torch
from timm import create_model
from timm.models.vision_transformer import VisionTransformer
from torch import nn
from torch.nn import functional as F

from models.layers import MLP, build_fusion_layer
from models.timm_wrapper import PatchEmbed
from models.sidenet_vit import *

class SideAdapterNetwork(nn.Module):
    def __init__(self, model_name, fusion_type, fusion_map, deep_supervision_idxs, depth=8):
        super().__init__()
        
        vit_model= create_model(
            model_name,
            False,
            img_size=224,
            drop_path_rate=0.0,
            fc_norm=False,
            num_classes=0,
            embed_layer=PatchEmbed,
        )
        self.depth = depth
        if vit_model.cls_token is not None:
            vit_model.pos_embed = nn.Parameter(vit_model.pos_embed[:, 1:, ...])
        del vit_model.cls_token
        vit_model.cls_token = None
        del vit_model.norm
        vit_model.norm = nn.Identity()
        self.vit_model = vit_model
        self.num_features = vit_model.num_features
        self.cls_embed = nn.Parameter(torch.zeros(1, 1, self.num_features))
        self.cls_pos_embed = nn.Parameter(
            torch.zeros(1, 1, self.num_features)
        )
        nn.init.normal_(self.cls_embed, std=0.02)
        nn.init.normal_(self.cls_pos_embed, std=0.02)

        self.fusion_map = {int(j): int(i) for i, j in [x.split("->") for x in fusion_map]}
        
        fusion_type: str = 'add'
        fusion_layers = nn.ModuleDict(
            {
                f"layer_{tgt_idx}": build_fusion_layer(
                    fusion_type, 768, vit_model.num_features
                )
                for tgt_idx, src_idx in self.fusion_map.items()
            }
        )
        self.fusion_layers = fusion_layers
        self.deep_supervision_idxs = deep_supervision_idxs

    def forward(
        self, image, clip_features):
        features, hydra_features= self.forward_features(image, clip_features)
        return features


    def forward_features(self, image, clip_features) :
        x, (h, w) = self.vit_model.patch_embed(image) 
        L = x.shape[1]  
        pos_embed = self.vit_model.pos_embed 
        ori_h, ori_w = self.vit_model.patch_embed.grid_size
        if pos_embed.shape[1] != L: 
            pos_embed = (
                F.interpolate(
                    pos_embed.reshape(1, ori_h, ori_w, -1).permute(0, 3, 1, 2),
                    size=[h, w],
                    mode="bicubic",
                    align_corners=False,
                )
                .flatten(2)
                .permute(0, 2, 1)
            )
        pos_embed = torch.cat(
            [self.cls_pos_embed.expand(pos_embed.shape[0], -1, -1), pos_embed], dim=1
        )
        x = torch.cat(
            [self.cls_embed.expand(x.shape[0], -1, -1), x],
            dim=1,
        )  
        x = x + pos_embed
        x = self.vit_model.norm_pre(x)
        hydra_x=[]
        x = self.fuse(0, x, clip_features, (h, w))
        hydra_x.append(x)
        outs = []
        
        for i, blk in enumerate(self.vit_model.blocks[:self.depth-1], start=1):
            x = blk(x)
            x = self.fuse(i, x, clip_features, (h, w))
            if i in self.fusion_map:
                hydra_x.append(x)
            if i in self.deep_supervision_idxs:
                outs = x
            if i < len(self.vit_model.blocks):
                x = x + pos_embed
        return outs, torch.stack(hydra_x)

    def fuse(self,block_idx,x,clip_features,spatial_shape,) :
        
        if block_idx in self.fusion_map: 
            src_idx = self.fusion_map[block_idx] 
            fusion_features = self.fusion_layers[f"layer_{block_idx}"](x[:, 1:, ...], clip_features[src_idx], spatial_shape)
            x = torch.cat(
                [
                    x[:, :1, ...],
                    fusion_features,
                ],
                dim=1,
            )
        return x

文件结束
新文件：models\sidenet_vit.py

文件开始
import torch
import torch.nn as nn
from timm.models.vision_transformer import Mlp, Block

class SidePatchEmbed(nn.Module):
    def __init__(self, img_size=224, in_chans=3, embed_dim=240):
        super(SidePatchEmbed, self).__init__()
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=16, stride=16)
        self.norm = nn.Identity()

    def forward(self, x):
        x = self.proj(x)
        x = self.norm(x)
        return x

def create_vit_block(dim, num_heads, mlp_dim, qkv_bias=False, drop=0.0, attn_drop=0.0, drop_path=0.0):
    block = nn.Sequential(
        nn.LayerNorm(dim),
        Block(
            dim=dim,
            num_heads=num_heads,
            mlp_ratio=mlp_dim / dim,
            qkv_bias=qkv_bias,
            attn_drop=attn_drop,
            drop_path=drop_path
        ),
        nn.Identity(),  
        nn.Identity()  
    )
    return block

class SideViT(nn.Module):
    def __init__(self, img_size, dim, num_heads, mlp_dim, depth, qkv_bias=False,
                 drop=0.0, attn_drop=0.0, drop_path=0.0,embed_layer=None):
        super(SideViT, self).__init__()
        self.num_features = dim
        if embed_layer :
            self.patch_embed = embed_layer(img_size=img_size, patch_size=16, in_chans=3, embed_dim=dim)
        else:
            self.patch_embed = SidePatchEmbed(img_size=img_size, in_chans=3, embed_dim=dim)
        self.blocks = nn.ModuleList([
            create_vit_block(dim, num_heads, mlp_dim, qkv_bias, drop, attn_drop, drop_path)
            for _ in range(depth)  
        ])

        self.head = nn.Sequential(
            nn.Identity(),  
            nn.Dropout(p=0.0, inplace=False),  
            nn.Identity()  
        )

    def forward(self, x):
        x = self.patch_embed(x)
        for block in self.blocks:
            x = block(x)
        x = self.head(x)

        return x
文件结束
新文件：models\spatial_sidenet.py

文件开始
import logging
from functools import partial
from typing import Dict, List, Tuple
import torch
from timm import create_model
from timm.models.vision_transformer import VisionTransformer
from torch import nn
from torch.nn import functional as F
from models.layers import MLP, build_fusion_layer
from models.timm_wrapper import PatchEmbed
from models.sidenet_vit import *

class TemporalSideAdapterNetwork(nn.Module):
    def __init__(self, fusion_type, fusion_map, deep_supervision_idxs):
        super().__init__()
        
        vit_model= create_model(
            'vit_base_patch16_224',
            False,
            img_size=224,
            drop_path_rate=0.0,
            fc_norm=False,
            num_classes=0,
            embed_layer=PatchEmbed,
        )
        if vit_model.cls_token is not None:
            vit_model.pos_embed = nn.Parameter(vit_model.pos_embed[:, 1:, ...])
        del vit_model.cls_token
        vit_model.cls_token = None
        del vit_model.norm
        vit_model.norm = nn.Identity()
        self.vit_model = vit_model

        self.num_features = vit_model.num_features
        self.cls_embed     = nn.Parameter(torch.zeros(1, 1, self.num_features))
        self.cls_pos_embed = nn.Parameter(torch.zeros(1, 1, self.num_features))

        nn.init.normal_(self.cls_embed, std=0.02)
        nn.init.normal_(self.cls_pos_embed, std=0.02)

        self.fusion_map = {int(j): int(i) for i, j in [x.split("->") for x in fusion_map]}

        fusion_type: str = 'add'
        fusion_layers = nn.ModuleDict(
            {
                f"layer_{tgt_idx}": build_fusion_layer(fusion_type, 768, vit_model.num_features)
                for tgt_idx, src_idx in self.fusion_map.items()
            }
        )

        temporal_fusion_layers = nn.ModuleDict(
            {
                f"layer_{tgt_idx}": build_fusion_layer('add1d', 768, vit_model.num_features)
                for tgt_idx, src_idx in self.fusion_map.items()
            }
        )

        self.fusion_layers = fusion_layers
        self.temporal_fusion_layers = temporal_fusion_layers
        self.deep_supervision_idxs = deep_supervision_idxs

    def forward(self, image, clip_features, PreFrameSideFeatures):
        features, hydra_features= self.forward_features(image, clip_features, PreFrameSideFeatures)
        return features, hydra_features

    def forward_features(self, image, clip_features, PreFrameSideFeatures) :
        x, (h, w) = self.vit_model.patch_embed(image) 
        L = x.shape[1]  
        pos_embed = self.vit_model.pos_embed 
        ori_h, ori_w = self.vit_model.patch_embed.grid_size
        if pos_embed.shape[1] != L: 
            pos_embed = (F.interpolate(pos_embed.reshape(1, ori_h, ori_w, -1).permute(0, 3, 1, 2), size=[h, w], mode="bicubic", align_corners=False,).flatten(2).permute(0, 2, 1))  
        pos_embed = torch.cat([self.cls_pos_embed.expand(pos_embed.shape[0], -1, -1), pos_embed], dim=1)
        x = torch.cat([self.cls_embed.expand(x.shape[0], -1, -1), x], dim=1,)  
        x = x + pos_embed
        x = self.vit_model.norm_pre(x)
        hydra_x=[]
        x = self.fuse(0, x, clip_features, PreFrameSideFeatures, (h, w))
        hydra_x.append(x)
        outs = []

        for i, blk in enumerate(self.vit_model.blocks, start=1): 
            x = blk(x)
            x = self.fuse(i, x, clip_features, PreFrameSideFeatures, (h, w)) 
            if i in self.fusion_map:
                hydra_x.append(x)
            if i in self.deep_supervision_idxs:
                outs = x
            if i < len(self.vit_model.blocks):
                x = x + pos_embed
        return outs, torch.stack(hydra_x)

    def fuse(self, block_idx, x, clip_features, PreFrameSideFeatures, spatial_shape) :
        if block_idx in self.fusion_map:    
            src_idx = self.fusion_map[block_idx] 
            if PreFrameSideFeatures is not None:
                fusion_features = self.fusion_layers[f"layer_{block_idx}"](x[:, 1:, ...], clip_features[src_idx], spatial_shape)
                temporal_fusion_features = self.temporal_fusion_layers[f"layer_{block_idx}"](fusion_features, PreFrameSideFeatures[block_idx])
                x = torch.cat([x[:, :1, ...], temporal_fusion_features], dim=1)
            else:
                fusion_features = self.fusion_layers[f"layer_{block_idx}"](x[:, 1:, ...], clip_features[src_idx], spatial_shape)
                x = torch.cat([x[:, :1, ...], fusion_features], dim=1)
        return x




文件结束
新文件：models\temporal_sidenet.py

文件开始
import logging
from functools import partial
from typing import Dict, List, Tuple

import torch
from timm import create_model
from timm.models.vision_transformer import VisionTransformer
from torch import nn
from torch.nn import functional as F

from models.layers import MLP, build_fusion_layer
from models.timm_wrapper import PatchEmbed
from models.sidenet_vit import *

class TemporalSideAdapterNetwork(nn.Module):
    def __init__(self, fusion_type, fusion_map, deep_supervision_idxs):
        super().__init__()
        vit_model= create_model(
            'vit_base_patch16_224',
            False,
            img_size=224,
            drop_path_rate=0.0,
            fc_norm=False,
            num_classes=0,
            embed_layer=PatchEmbed,
        )
        if vit_model.cls_token is not None:
            vit_model.pos_embed = nn.Parameter(vit_model.pos_embed[:, 1:, ...])
        del vit_model.cls_token
        vit_model.cls_token = None
        del vit_model.norm
        vit_model.norm = nn.Identity()
        self.vit_model = vit_model
        self.num_features = vit_model.num_features
        self.cls_embed     = nn.Parameter(torch.zeros(1, 1, self.num_features))
        self.cls_pos_embed = nn.Parameter(torch.zeros(1, 1, self.num_features))

        nn.init.normal_(self.cls_embed, std=0.02)
        nn.init.normal_(self.cls_pos_embed, std=0.02)

        self.fusion_map = {int(j): int(i) for i, j in [x.split("->") for x in fusion_map]}

        fusion_type: str = 'add'
        fusion_layers = nn.ModuleDict(
            {
                f"layer_{tgt_idx}": build_fusion_layer(fusion_type, 768, vit_model.num_features)
                for tgt_idx, src_idx in self.fusion_map.items()
            }
        )

        temporal_fusion_layers = nn.ModuleDict(
            {
                f"layer_{tgt_idx}": build_fusion_layer('add1d', 768, vit_model.num_features)
                for tgt_idx, src_idx in self.fusion_map.items()
            }
        )

        self.fusion_layers = fusion_layers
        self.temporal_fusion_layers = temporal_fusion_layers
        self.deep_supervision_idxs = deep_supervision_idxs

    def forward(self, image, clip_features, PreFrameSideFeatures):
        features, hydra_features= self.forward_features(image, clip_features, PreFrameSideFeatures)
        return features, hydra_features

    def forward_features(self, image, clip_features, PreFrameSideFeatures) :
        x, (h, w) = self.vit_model.patch_embed(image) 
        L = x.shape[1]  
        pos_embed = self.vit_model.pos_embed 
        ori_h, ori_w = self.vit_model.patch_embed.grid_size
        if pos_embed.shape[1] != L: 
            pos_embed = (F.interpolate(pos_embed.reshape(1, ori_h, ori_w, -1).permute(0, 3, 1, 2), size=[h, w], mode="bicubic", align_corners=False,).flatten(2).permute(0, 2, 1)) 
        pos_embed = torch.cat([self.cls_pos_embed.expand(pos_embed.shape[0], -1, -1), pos_embed], dim=1)

        x = torch.cat([self.cls_embed.expand(x.shape[0], -1, -1), x], dim=1,)  
        x = x + pos_embed
        x = self.vit_model.norm_pre(x)
        hydra_x=[]

        x = self.fuse(0, x, clip_features, PreFrameSideFeatures, (h, w))
        hydra_x.append(x)
        outs = []
        
        for i, blk in enumerate(self.vit_model.blocks, start=1): 
            x = blk(x)
            x = self.fuse(i, x, clip_features, PreFrameSideFeatures, (h, w)) 
            if i in self.fusion_map:
                hydra_x.append(x)
            if i in self.deep_supervision_idxs:
                outs = x
            if i < len(self.vit_model.blocks):
                x = x + pos_embed
        return outs, torch.stack(hydra_x)

    def fuse(self, block_idx, x, clip_features, PreFrameSideFeatures, spatial_shape) :
        if block_idx in self.fusion_map:    
            src_idx = self.fusion_map[block_idx] 
            if PreFrameSideFeatures is not None:
                fusion_features = self.fusion_layers[f"layer_{block_idx}"](x[:, 1:, ...], clip_features[src_idx], spatial_shape)
                temporal_fusion_features = self.temporal_fusion_layers[f"layer_{block_idx}"](fusion_features, PreFrameSideFeatures[block_idx])
                x = torch.cat([x[:, :1, ...], temporal_fusion_features], dim=1)
            else:
                fusion_features = self.fusion_layers[f"layer_{block_idx}"](x[:, 1:, ...], clip_features[src_idx], spatial_shape)
                x = torch.cat([x[:, :1, ...], fusion_features], dim=1)
        return x




文件结束
新文件：models\test_base.py

文件开始
import torch.nn as nn
import torch
from CLIP.clip import clip
from models.vit import *
from models.visual import *
from models.sidenet import *
from models.sidenet_vit import *
from models.temporal_sidenet import *
from models.CrossFrameSidenet import *
from config import argument_parser

parser = argument_parser()
args = parser.parse_args()
device = "cuda" if torch.cuda.is_available() else "cpu"
model_register = {144: 'vit_w144n6d8_patch16', 240: 'vit_w240n6d8_patch16', 256: 'vit_w256n8d8_patch16',
                336: 'vit_w336n6d8_patch16', 432: 'vit_w432n6d8_patch16', 512: 'vit_w512n8d8_patch16',
                768: 'vit_w768n8d8_patch16'}


class TransformerClassifier(nn.Module):
    def __init__(self, ViT_model, attr_num,attr_words, dim=args.dim):
        super().__init__()
        super().__init__()
        self.attr_num = attr_num
        model_name = model_register[args.dim]
        self.clip_visual_extractor = FeatureExtractor(ViT_model.visual, last_layer_idx=-1, frozen_exclude=["positional_embedding"],)
        self.spatial_sidenet = SideAdapterNetwork(model_name,args.fusion_type, args.fusion_map, [7, 8])
        self.crossframe_sidenet = CrossFrameSideNetwork(model_name,args.fusion_type, args.fusion_map, [7, 8])
        self.img_type = ViT_model.dtype
        self.word_embed = nn.Linear(512, dim)
        self.visual_embed= nn.Linear(768, dim)
       
        vit_model= create_model(
            model_name,
            False,
            img_size=224,
            drop_path_rate=0.0,
            fc_norm=False,
            num_classes=0,
            embed_layer=PatchEmbed,
        )

        self.blocks = vit_model.blocks[-1:]
        self.norm = vit_model.norm
        self.weight_layer = nn.ModuleList([nn.Linear(dim, 1) for i in range(self.attr_num)])
        self.bn = nn.BatchNorm1d(self.attr_num)
        self.text = clip.tokenize(attr_words).to(device)
        self.fusion_linear = nn.Linear(197*2, 197) 
        self.w_temporal = nn.Parameter(torch.ones(1))
        input_size = 197 * dim
        self.spatial_lstm = nn.GRU(input_size=dim, hidden_size=dim, num_layers=1, batch_first=True)
        self.temporal_lstm = nn.GRU(input_size=dim, hidden_size=dim, num_layers=1, batch_first=True)

    def forward(self, videos,ViT_model):
        ViT_features=[]
        if len(videos.size())<5 :
            videos.unsqueeze(1) 

        batch_size, num_frames, channels, height, width = videos.size()
        imgs=videos.view(-1, channels, height, width)
        clip_image_features, _ = self.clip_visual_extractor(imgs.type(self.img_type))
        spatial_features = self.spatial_sidenet(imgs,clip_image_features)
        _,L,side_D = spatial_features.size()
        spatial_features = spatial_features.view(batch_size,num_frames,L,side_D)
        fuse_spatial_feat = spatial_features.mean(dim=1)
        frame_select_feat={k:[] for k in range(num_frames)}

        for layer in self.spatial_sidenet.fusion_map.values():
            selected_layer_feat = clip_image_features[layer]
            BF,D,H,W = selected_layer_feat.shape
            reshape_feat=selected_layer_feat.view(BF,D,-1).permute(0,2,1).view(batch_size,num_frames,H*W,D).permute(1,0,2,3)
            for fidx,frame_feat in enumerate(reshape_feat) :
                frame_select_feat[fidx].append(frame_feat.cuda().float())
        
        temporal_features = self.crossframe_sidenet(frame_select_feat)
        fuse_temporal_feat = temporal_features.mean(dim=0)
        fusion_side_feat = fuse_spatial_feat + fuse_temporal_feat
        text_features = ViT_model.encode_text(self.text).to(device).float()
        textual_features = self.word_embed(text_features).expand(batch_size, text_features.shape[0],side_D)  
        x = torch.cat([textual_features, fusion_side_feat], dim=1)
        for b_c, blk in enumerate(self.blocks):
            x,attn_map = blk(x)
        x = self.norm(x)

        logits = torch.cat([self.weight_layer[i](x[:, i, :]) for i in range(self.attr_num)], dim=1)
        logits = self.bn(logits)
        
        return logits
    

文件结束
新文件：models\timm_wrapper.py

文件开始
import warnings
from torch import nn
from timm.models.vision_transformer import _create_vision_transformer
from timm.models import register_model
from timm.models.layers import to_2tuple


class PatchEmbed(nn.Module):
    """2D Image to Patch Embedding. Modify the original implementation to allow return the 2D patch size."""

    def __init__(
        self,
        img_size=224,
        patch_size=16,
        in_chans=3,
        embed_dim=768,
        norm_layer=None,
        flatten=True,
        bias=True,
        **kwargs
    ):
        super().__init__()
        if len(kwargs)>0:
            warnings.warn(f"Unused kwargs are provided:{kwargs}.")
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        self.flatten = flatten

        self.proj = nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias
        )
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def forward(self, x):
        x = self.proj(x)
        h, w = x.shape[-2:]
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        x = self.norm(x)
        return x, (h, w)


@register_model
def vit_w144n6d8_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=144, depth=8, num_heads=6, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model


@register_model
def vit_w240n6d8_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=240, depth=8, num_heads=6, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model

@register_model
def vit_w240n6d10_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=240, depth=10, num_heads=6, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model

@register_model
def vit_w256n8d8_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=256, depth=8, num_heads=8, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model

@register_model
def vit_w336n6d8_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=336, depth=8, num_heads=6, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model

@register_model
def vit_w432n6d8_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=432, depth=8, num_heads=6, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model

@register_model
def vit_w512n8d8_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=512, depth=8, num_heads=8, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model

@register_model
def vit_w768n8d8_patch16(pretrained=False, **kwargs):
    assert not pretrained
    model_kwargs = dict(patch_size=16, embed_dim=768, depth=8, num_heads=8, **kwargs)
    model = _create_vision_transformer(
        "vit_tiny_patch16_224_in21k", pretrained=pretrained, **model_kwargs
    )
    return model
文件结束
新文件：models\visual.py

文件开始
from typing import List
import torch
from torch import nn
from torch.nn import functional as F
from open_clip.transformer import VisionTransformer
from models.attn_helper import cross_attn_layer, downsample2d, resize_pos_embed2d

class ClipOutput(dict):
    def __init__(self, spacial_shape, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spacial_shape = spacial_shape

    def save(self, idx: int, clip_feat: torch.Tensor):
        l, n, c = clip_feat.shape
        self[idx] = (
            clip_feat[1:].permute(1, 2, 0).reshape(n, c, *self.spacial_shape)
        )  # n, c, h, w
        self[f"{idx}_cls_token"] = clip_feat[0:1]  # 1, n, c


class FeatureExtractor(nn.Module):
    
    def __init__(
        self,
        visual_encoder: VisionTransformer,
        last_layer_idx: int = -1,
        frozen_exclude=[],
    ):
        super().__init__()
        self.image_size = visual_encoder.input_resolution
        self.patch_size = visual_encoder.patch_size
        self.grid_size = self.image_size // self.patch_size
        self.num_features = visual_encoder.ln_pre.normalized_shape[0]#768
        self.conv1 = visual_encoder.conv1
        self.class_embedding = visual_encoder.class_embedding
        self.positional_embedding = visual_encoder.positional_embedding
        self.ln_pre = visual_encoder.ln_pre
        if last_layer_idx == -1:
            self.resblocks = visual_encoder.transformer.resblocks
            self.last_output_idx = len(self.resblocks) + 1
        else:
            self.resblocks = visual_encoder.transformer.resblocks[:last_layer_idx]
            self.last_output_idx = last_layer_idx + 1
        self.frozen_exclude = frozen_exclude
        self._freeze(self.frozen_exclude)

    def forward(self, x: torch.Tensor):
        x = self.conv1(x)  
        _, _, h, w = x.shape
        x = x.reshape(x.shape[0], x.shape[1], -1)  
        x = x.permute(0, 2, 1)  
        x = torch.cat(
            [
                self.class_embedding.to(x.dtype)
                + torch.zeros(
                    x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device
                ),
                x,
            ],
            dim=1,
        ) 
        x = x + self.positional_embedding.to(x.dtype)
        x = self.ln_pre(x)
        x = x.permute(1, 0, 2)  # NLD -> LND

        outputs = ClipOutput(spacial_shape=(h, w))
        outputs.save(0, x)
        
        for i, resblock in enumerate(self.resblocks, start=1):
            x = resblock(x)
            outputs.save(i, x)
        return outputs,x

    def _freeze(self, frozen_exclude):
        if "all" in frozen_exclude:
            return
        for name, param in self.named_parameters():
            if not any([exclude in name for exclude in frozen_exclude]):
                param.requires_grad = False

    @property
    def size_divisibility(self):
        return self.patch_size[0]
文件结束
新文件：models\vit.py

文件开始
""" Vision Transformer (ViT) in PyTorch

A PyTorch implement of Vision Transformers as described in
'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929

The official jax code is released and available at https://github.com/google-research/vision_transformer

Status/TODO:
* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.
* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.
* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.
* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.

Acknowledgments:
* The paper authors for releasing code and weights, thanks!
* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out
for some einops/einsum fun
* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT
* Bert reference code checks against Huggingface Transformers and Tensorflow Bert

Hacked together by / Copyright 2020 Ross Wightman
"""
import math
from functools import reduce
from operator import mul
from functools import partial
from itertools import repeat

import torch
import torch.nn as nn
import torch.nn.functional as F
import collections.abc as container_abcs

# From PyTorch internals
def _ntuple(n):
    def parse(x):
        if isinstance(x, container_abcs.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse

IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)
IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)
to_2tuple = _ntuple(2)

def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output

class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


def _cfg(url='', **kwargs):
    return {
        'url': url,
        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
        'crop_pct': .9, 'interpolation': 'bicubic',
        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
        'first_conv': 'patch_embed.proj', 'classifier': 'head',
        **kwargs
    }


default_cfgs = {
    # patch models
    'vit_small_patch16_224': _cfg(
        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',
    ),
    'vit_base_patch16_224': _cfg(
        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',
        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
    ),
    'vit_base_patch16_384': _cfg(
        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',
        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),
    'vit_base_patch32_384': _cfg(
        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',
        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),
    'vit_large_patch16_224': _cfg(
        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',
        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    'vit_large_patch16_384': _cfg(
        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',
        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),
    'vit_large_patch32_384': _cfg(
        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',
        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),
    'vit_huge_patch16_224': _cfg(),
    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),
    # hybrid models
    'vit_small_resnet26d_224': _cfg(),
    'vit_small_resnet50d_s3_224': _cfg(),
    'vit_base_resnet26d_224': _cfg(),
    'vit_base_resnet50d_224': _cfg(),
}


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
    def forward(self, x):#这里加一下
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=16, stride_size=20, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        stride_size_tuple = to_2tuple(stride_size)
        self.num_x = (img_size[1] - patch_size[1]) // stride_size_tuple[1] + 1
        self.num_y = (img_size[0] - patch_size[0]) // stride_size_tuple[0] + 1

        num_patches = self.num_x * self.num_y
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride_size)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.InstanceNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, x):
        B, C, H, W = x.shape

        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x)

        x = x.flatten(2).transpose(1, 2) # [64, 8, 768]
        return x


class ViT(nn.Module):
    def __init__(self, img_size=224, patch_size=16, stride_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,
                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):
        super().__init__()
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.patch_embed = PatchEmbed(
                img_size=img_size, patch_size=patch_size, stride_size=stride_size, in_chans=in_chans,
                embed_dim=embed_dim)

        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        
        self.pos_drop = nn.Dropout(p=drop_rate)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule

        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
            for i in range(depth)]) #block
        self.norm = norm_layer(embed_dim)

        # Classifier head
        self.fc = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        trunc_normal_(self.cls_token, std=.02)
        trunc_normal_(self.pos_embed, std=.02)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)

        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
        x = torch.cat((cls_tokens, x), dim=1) + self.pos_embed
        x = self.pos_drop(x)

        for blk in self.blocks[:-1]:
            x = blk(x)
        return x

    def load_param(self, model_path):
        param_dict = torch.load(model_path, map_location='cpu')
        if 'model' in param_dict:
            param_dict = param_dict['model']
        if 'state_dict' in param_dict:
            param_dict = param_dict['state_dict']
        for k, v in param_dict.items():
            if 'head' in k or 'dist' in k:
                continue
            if 'patch_embed.proj.weight' in k and len(v.shape) < 4:
                # For old models that I trained prior to conv based patchification
                O, I, H, W = self.patch_embed.proj.weight.shape
                v = v.reshape(O, -1, H, W)
            elif k == 'pos_embed' and v.shape != self.pos_embed.shape:
                v = resize_pos_embed(v, self.pos_embed, self.patch_embed.num_y, self.patch_embed.num_x)
            try:
                self.state_dict()[k].copy_(v)
            except:
                print('===========================ERROR=========================')
                print('shape do not match in k :{}: param_dict{} vs self.state_dict(){}'.format(k, v.shape, self.state_dict()[k].shape))


def resize_pos_embed(posemb, posemb_new, hight, width):
    # Rescale the grid of position embeddings when loading from state_dict. Adapted from
    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224
    ntok_new = posemb_new.shape[1]

    posemb_token, posemb_grid = posemb[:, :1], posemb[0, 1:]
    ntok_new -= 1

    gs_old = int(math.sqrt(len(posemb_grid)))
    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)
    posemb_grid = F.interpolate(posemb_grid, size=(hight, width), mode='bilinear', align_corners=False)
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, hight * width, -1)
    posemb = torch.cat([posemb_token, posemb_grid], dim=1)
    return posemb


def vit_base(img_size=(224, 224), stride_size=16, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, **kwargs):
    model = ViT(
        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\
        drop_path_rate=drop_path_rate, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,
        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

文件结束
新文件：models\__init__.py

文件开始


文件结束
新文件：solver\cosine_lr.py

文件开始
""" Cosine Scheduler

Cosine LR schedule with warmup, cycle/restarts, noise.

Hacked together by / Copyright 2020 Ross Wightman
"""
import logging
import math
import torch

from .scheduler import Scheduler


_logger = logging.getLogger(__name__)


class CosineLRScheduler(Scheduler):
    """
    Cosine decay with restarts.
    This is described in the paper https://arxiv.org/abs/1608.03983.

    Inspiration from
    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py
    """

    def __init__(self,
                 optimizer: torch.optim.Optimizer,
                 t_initial: int,
                 t_mul: float = 1.,
                 lr_min: float = 0.,
                 decay_rate: float = 1.,
                 warmup_t=0,
                 warmup_lr_init=0,
                 warmup_prefix=False,
                 cycle_limit=0,
                 t_in_epochs=True,
                 noise_range_t=None,
                 noise_pct=0.67,
                 noise_std=1.0,
                 noise_seed=42,
                 initialize=True) -> None:
        super().__init__(
            optimizer, param_group_field="lr",
            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,
            initialize=initialize)

        assert t_initial > 0
        assert lr_min >= 0
        if t_initial == 1 and t_mul == 1 and decay_rate == 1:
            _logger.warning("Cosine annealing scheduler will have no effect on the learning "
                           "rate since t_initial = t_mul = eta_mul = 1.")
        self.t_initial = t_initial
        self.t_mul = t_mul
        self.lr_min = lr_min
        self.decay_rate = decay_rate
        self.cycle_limit = cycle_limit
        self.warmup_t = warmup_t
        self.warmup_lr_init = warmup_lr_init
        self.warmup_prefix = warmup_prefix
        self.t_in_epochs = t_in_epochs
        if self.warmup_t:
            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]
            super().update_groups(self.warmup_lr_init)
        else:
            self.warmup_steps = [1 for _ in self.base_values]

    def _get_lr(self, t):
        if t < self.warmup_t:
            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]
        else:
            if self.warmup_prefix:
                t = t - self.warmup_t

            if self.t_mul != 1:
                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.t_mul), self.t_mul))
                t_i = self.t_mul ** i * self.t_initial
                t_curr = t - (1 - self.t_mul ** i) / (1 - self.t_mul) * self.t_initial
            else:
                i = t // self.t_initial
                t_i = self.t_initial
                t_curr = t - (self.t_initial * i)

            gamma = self.decay_rate ** i
            lr_min = self.lr_min * gamma
            lr_max_values = [v * gamma for v in self.base_values]

            if self.cycle_limit == 0 or (self.cycle_limit > 0 and i < self.cycle_limit):
                lrs = [
                    lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * t_curr / t_i)) for lr_max in lr_max_values
                ]
            else:
                lrs = [self.lr_min for _ in self.base_values]

        return lrs

    def get_epoch_values(self, epoch: int):
        if self.t_in_epochs:
            return self._get_lr(epoch)
        else:
            return None

    def get_update_values(self, num_updates: int):
        if not self.t_in_epochs:
            return self._get_lr(num_updates)
        else:
            return None

    def get_cycle_length(self, cycles=0):
        if not cycles:
            cycles = self.cycle_limit
        cycles = max(1, cycles)
        if self.t_mul == 1.0:
            return self.t_initial * cycles
        else:
            return int(math.floor(-self.t_initial * (self.t_mul ** cycles - 1) / (1 - self.t_mul)))

文件结束
新文件：solver\lr_scheduler.py

文件开始
# encoding: utf-8
"""
@author:  liaoxingyu
@contact: sherlockliao01@gmail.com
"""
from bisect import bisect_right
import torch


# FIXME ideally this would be achieved with a CombinedLRScheduler,
# separating MultiStepLR with WarmupLR
# but the current LRScheduler design doesn't allow it

class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):
    def __init__(
            self,
            optimizer,
            milestones,  # steps
            gamma=0.1,
            warmup_factor=1.0 / 3,
            warmup_iters=500,
            warmup_method="linear",
            last_epoch=-1,
    ):
        if not list(milestones) == sorted(milestones):
            raise ValueError(
                "Milestones should be a list of" " increasing integers. Got {}",
                milestones,
            )

        if warmup_method not in ("constant", "linear"):
            raise ValueError(
                "Only 'constant' or 'linear' warmup_method accepted"
                "got {}".format(warmup_method)
            )
        self.milestones = milestones
        self.gamma = gamma
        self.warmup_factor = warmup_factor
        self.warmup_iters = warmup_iters
        self.warmup_method = warmup_method
        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)

    def _get_lr(self):
        warmup_factor = 1
        if self.last_epoch < self.warmup_iters:
            if self.warmup_method == "constant":
                warmup_factor = self.warmup_factor
            elif self.warmup_method == "linear":
                alpha = self.last_epoch / self.warmup_iters
                warmup_factor = self.warmup_factor * (1 - alpha) + alpha
        return [
            base_lr
            * warmup_factor
            * self.gamma ** bisect_right(self.milestones, self.last_epoch)
            for base_lr in self.base_lrs
        ]

文件结束
新文件：solver\make_optimizer.py

文件开始
import torch


def make_optimizer(model, lr=8e-3, weight_decay=1e-4, momentum=0.9):
    params = []
    for key, value in model.named_parameters():
        if not value.requires_grad:
            continue
        params += [{"params": [value], "lr": lr, "weight_decay": weight_decay}]

    optimizer = getattr(torch.optim, 'SGD')(params, momentum=momentum)
    
    return optimizer

文件结束
新文件：solver\scheduler.py

文件开始
from typing import Dict, Any

import torch


class Scheduler:
    """ Parameter Scheduler Base Class
    A scheduler base class that can be used to schedule any optimizer parameter groups.

    Unlike the builtin PyTorch schedulers, this is intended to be consistently called
    * At the END of each epoch, before incrementing the epoch count, to calculate next epoch's value
    * At the END of each optimizer update, after incrementing the update count, to calculate next update's value

    The schedulers built on this should try to remain as stateless as possible (for simplicity).

    This family of schedulers is attempting to avoid the confusion of the meaning of 'last_epoch'
    and -1 values for special behaviour. All epoch and update counts must be tracked in the training
    code and explicitly passed in to the schedulers on the corresponding step or step_update call.

    Based on ideas from:
     * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler
     * https://github.com/allenai/allennlp/tree/master/allennlp/training/learning_rate_schedulers
    """

    def __init__(self,
                 optimizer: torch.optim.Optimizer,
                 param_group_field: str,
                 noise_range_t=None,
                 noise_type='normal',
                 noise_pct=0.67,
                 noise_std=1.0,
                 noise_seed=None,
                 initialize: bool = True) -> None:
        self.optimizer = optimizer
        self.param_group_field = param_group_field
        self._initial_param_group_field = f"initial_{param_group_field}"
        if initialize:
            for i, group in enumerate(self.optimizer.param_groups):
                if param_group_field not in group:
                    raise KeyError(f"{param_group_field} missing from param_groups[{i}]")
                group.setdefault(self._initial_param_group_field, group[param_group_field])
        else:
            for i, group in enumerate(self.optimizer.param_groups):
                if self._initial_param_group_field not in group:
                    raise KeyError(f"{self._initial_param_group_field} missing from param_groups[{i}]")
        self.base_values = [group[self._initial_param_group_field] for group in self.optimizer.param_groups]
        self.metric = None  # any point to having this for all?
        self.noise_range_t = noise_range_t
        self.noise_pct = noise_pct
        self.noise_type = noise_type
        self.noise_std = noise_std
        self.noise_seed = noise_seed if noise_seed is not None else 42
        self.update_groups(self.base_values)

    def state_dict(self) -> Dict[str, Any]:
        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}

    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
        self.__dict__.update(state_dict)

    def get_epoch_values(self, epoch: int):
        return None

    def get_update_values(self, num_updates: int):
        return None

    def step(self, epoch: int, metric: float = None) -> None:
        self.metric = metric
        values = self.get_epoch_values(epoch)
        if values is not None:
            values = self._add_noise(values, epoch)
            self.update_groups(values)

    def step_update(self, num_updates: int, metric: float = None):
        self.metric = metric
        values = self.get_update_values(num_updates)
        if values is not None:
            values = self._add_noise(values, num_updates)
            self.update_groups(values)

    def update_groups(self, values):
        if not isinstance(values, (list, tuple)):
            values = [values] * len(self.optimizer.param_groups)
        for param_group, value in zip(self.optimizer.param_groups, values):
            param_group[self.param_group_field] = value

    def _add_noise(self, lrs, t):
        if self.noise_range_t is not None:
            if isinstance(self.noise_range_t, (list, tuple)):
                apply_noise = self.noise_range_t[0] <= t < self.noise_range_t[1]
            else:
                apply_noise = t >= self.noise_range_t
            if apply_noise:
                g = torch.Generator()
                g.manual_seed(self.noise_seed + t)
                if self.noise_type == 'normal':
                    while True:
                        # resample if noise out of percent limit, brute force but shouldn't spin much
                        noise = torch.randn(1, generator=g).item()
                        if abs(noise) < self.noise_pct:
                            break
                else:
                    noise = 2 * (torch.rand(1, generator=g).item() - 0.5) * self.noise_pct
                lrs = [v + v * noise for v in lrs]
        return lrs

文件结束
新文件：solver\scheduler_factory.py

文件开始
""" Scheduler Factory
Hacked together by / Copyright 2020 Ross Wightman
"""
from .cosine_lr import CosineLRScheduler


def create_scheduler(optimizer, num_epochs, lr=8e-3, warmup_t=10):
    lr_min = 0.002 * lr
    warmup_lr_init = 0.01 * lr
    noise_range = None

    lr_scheduler = CosineLRScheduler(
            optimizer,
            t_initial=num_epochs,
            lr_min=lr_min,
            t_mul= 1.,
            decay_rate=0.1,
            warmup_lr_init=warmup_lr_init,
            warmup_t=warmup_t,
            cycle_limit=1,
            t_in_epochs=True,
            noise_range_t=noise_range,
            noise_pct= 0.67,
            noise_std= 1.,
            noise_seed=42,
        )

    return lr_scheduler
def make_scheduler(optimizer, num_epochs, lr=8e-3, warmup_t=10):
    lr_min = 0.002 * lr
    warmup_lr_init = 0.01 * lr
    noise_range = None

    lr_scheduler = CosineLRScheduler(
            optimizer,
            t_initial=num_epochs,
            lr_min=lr_min,
            t_mul= 1.,
            decay_rate=0.1,
            warmup_lr_init=warmup_lr_init,
            warmup_t=warmup_t,
            cycle_limit=1,
            t_in_epochs=True,
            noise_range_t=noise_range,
            noise_pct= 0.67,
            noise_std= 1.,
            noise_seed=42,
        )

    return lr_scheduler

文件结束
新文件：solver\__init__.py

文件开始
from .lr_scheduler import WarmupMultiStepLR
from .make_optimizer import make_optimizer
文件结束
新文件：tools\function.py

文件开始
import os
from collections import OrderedDict

import numpy as np
import torch
from easydict import EasyDict

from tools.utils import may_mkdirs


def ratio2weight(targets, ratio):
    ratio = torch.from_numpy(ratio).type_as(targets)
    pos_weights = targets * (1 - ratio)
    neg_weights = (1 - targets) * ratio
    weights = torch.exp(neg_weights + pos_weights)
    weights[targets > 1] = 0.0

    return weights

class LogVisual:

    def __init__(self, args):
        self.args = vars(args)
        self.train_loss = []
        self.val_loss = []

        self.ap = []
        self.map = []
        self.acc = []
        self.prec = []
        self.recall = []
        self.f1 = []

        self.error_num = []
        self.fn_num = []
        self.fp_num = []

        self.save = False

    def append(self, **kwargs):
        self.save = False

        if 'result' in kwargs:
            self.ap.append(kwargs['result']['label_acc'])
            self.map.append(np.mean(kwargs['result']['label_acc']))
            self.acc.append(np.mean(kwargs['result']['instance_acc']))
            self.prec.append(np.mean(kwargs['result']['instance_precision']))
            self.recall.append(np.mean(kwargs['result']['instance_recall']))
            self.f1.append(np.mean(kwargs['result']['floatance_F1']))

            self.error_num.append(kwargs['result']['error_num'])
            self.fn_num.append(kwargs['result']['fn_num'])
            self.fp_num.append(kwargs['result']['fp_num'])

        if 'train_loss' in kwargs:
            self.train_loss.append(kwargs['train_loss'])
        if 'val_loss' in kwargs:
            self.val_loss.append(kwargs['val_loss'])


def get_pkl_rootpath(dataset):
    root = os.path.join("/dataset", f"{dataset}")
    data_path = os.path.join(root, 'dataset.pkl')

    return data_path


def get_pedestrian_metrics(gt_label, preds_probs, threshold=0.45):
    pred_label = preds_probs > threshold

    eps = 1e-20
    result = EasyDict()

    ###############################
    # label metrics
    # TP + FN
    gt_pos = np.sum((gt_label == 1), axis=0).astype(float)
    # TN + FP
    gt_neg = np.sum((gt_label == 0), axis=0).astype(float)
    # TP
    true_pos = np.sum((gt_label == 1) * (pred_label == 1), axis=0).astype(float)
    # TN
    true_neg = np.sum((gt_label == 0) * (pred_label == 0), axis=0).astype(float)
    # FP
    false_pos = np.sum(((gt_label == 0) * (pred_label == 1)), axis=0).astype(float)
    # FN
    false_neg = np.sum(((gt_label == 1) * (pred_label == 0)), axis=0).astype(float)

    label_pos_recall = 1.0 * true_pos / (gt_pos + eps)  # true positive
    label_neg_recall = 1.0 * true_neg / (gt_neg + eps)  # true negative
    # mean accuracy
    label_ma = (label_pos_recall + label_neg_recall) / 2

    result.label_pos_recall = label_pos_recall
    result.label_neg_recall = label_neg_recall
    result.label_prec = true_pos / (true_pos + false_pos + eps)
    result.label_acc = true_pos / (true_pos + false_pos + false_neg + eps)
    result.label_f1 = 2 * result.label_prec * result.label_pos_recall / (
            result.label_prec + result.label_pos_recall + eps)

    result.label_ma = label_ma
    result.ma = np.mean(label_ma)

    ################
    # instance metrics
    gt_pos = np.sum((gt_label == 1), axis=1).astype(float)
    true_pos = np.sum((pred_label == 1), axis=1).astype(float)
    # true positive
    intersect_pos = np.sum((gt_label == 1) * (pred_label == 1), axis=1).astype(float)
    # IOU
    union_pos = np.sum(((gt_label == 1) + (pred_label == 1)), axis=1).astype(float)

    instance_acc = intersect_pos / (union_pos + eps)
    instance_prec = intersect_pos / (true_pos + eps)
    instance_recall = intersect_pos / (gt_pos + eps)
    instance_f1 = 2 * instance_prec * instance_recall / (instance_prec + instance_recall + eps)

    instance_acc = np.mean(instance_acc)
    instance_prec = np.mean(instance_prec)
    instance_recall = np.mean(instance_recall)
    instance_f1 = np.mean(instance_f1)

    result.instance_acc = instance_acc
    result.instance_prec = instance_prec
    result.instance_recall = instance_recall
    result.instance_f1 = instance_f1

    result.error_num, result.fn_num, result.fp_num = false_pos + false_neg, false_neg, false_pos

    return result

def get_signle_metrics(gt_label, preds_probs, threshold=0.45):
    pred_label = preds_probs > threshold

    eps = 1e-20
    result = EasyDict()

    ###############################
    # label metrics
    # TP + FN
    gt_pos = np.sum((gt_label == 1)).astype(float)
    # TN + FP
    gt_neg = np.sum((gt_label == 0)).astype(float)
    # TP
    true_pos = np.sum((gt_label == 1) * (pred_label == 1)).astype(float)
    # TN
    true_neg = np.sum((gt_label == 0) * (pred_label == 0)).astype(float)
    # FP
    false_pos = np.sum(((gt_label == 0) * (pred_label == 1))).astype(float)
    # FN
    false_neg = np.sum(((gt_label == 1) * (pred_label == 0))).astype(float)

    label_pos_recall = 1.0 * true_pos / (gt_pos + eps)  # true positive
    label_neg_recall = 1.0 * true_neg / (gt_neg + eps)  # true negative
    # mean accuracy
    label_ma = (label_pos_recall + label_neg_recall) / 2

    result.label_pos_recall = label_pos_recall
    result.label_neg_recall = label_neg_recall
    result.label_prec = true_pos / (true_pos + false_pos + eps)
    result.label_acc = true_pos / (true_pos + false_pos + false_neg + eps)
    result.label_f1 = 2 * result.label_prec * result.label_pos_recall / (
            result.label_prec + result.label_pos_recall + eps)

    result.label_ma = label_ma
    result.ma = np.mean(label_ma)

    result.instance_f1 = np.mean(result.label_f1)
    result.instance_acc = np.mean(label_ma)
    result.instance_prec = np.mean(result.label_prec)
    result.instance_recall = np.mean(result.label_pos_recall)

    #result.error_num, result.fn_num, result.fp_num = false_pos + false_neg, false_neg, false_pos

    return result

def simple_par_metrics(gt_label, preds_probs, threshold=0.45, signle=False):
    pred_label = preds_probs > threshold
        
    eps = 1e-20
    result = EasyDict()
    if signle:
        preds_probs = preds_probs[:,np.newaxis]
    # TP
    TP = np.sum((gt_label == 1) * (pred_label == 1), axis=0).astype(float) 
    # TN
    TN = np.sum((gt_label == 0) * (pred_label == 0), axis=0).astype(float)
    # FP
    FP = np.sum(((gt_label == 0) * (pred_label == 1)), axis=0).astype(float)
    # FN
    FN = np.sum(((gt_label == 1) * (pred_label == 0)), axis=0).astype(float)

    TP =  np.sum(TP).astype(float)
    TN =  np.sum(TN).astype(float)
    FP =  np.sum(FP).astype(float)
    FN =  np.sum(FN).astype(float)

    acc = (TP + TN) / (TP + TN + FP + FN + eps)
    prec = (TP) / (TP + FP + eps)
    recall = (TP) / (TP + FN + eps)
    f1 = 2 * prec * recall / (prec + recall + eps)
    
    result.acc = acc
    result.prec = prec
    result.recall = recall
    result.f1 = f1

    return result



文件结束
新文件：tools\utils.py

文件开始
import os
import pickle
import datetime
import time
# from contextlib import contextmanger
import torch
from torch.autograd import Variable
import random
import numpy as np


def time_str(fmt=None):
    if fmt is None:
        fmt = '%Y-%m-%d_%H_%M_%S'

    #     time.strftime(format[, t])
    return datetime.datetime.today().strftime(fmt)


def str2bool(v):
    return v.lower() in ("yes", "true", "1")


def is_iterable(obj):
    return hasattr(obj, '__len__')


def to_scalar(vt):
    """
    preprocess a 1-length pytorch Variable or Tensor to scalar
    """
    # if isinstance(vt, Variable):
    #     return vt.data.cpu().numpy().flatten()[0]
    if torch.is_tensor(vt):
        if vt.dim() == 0:
            return vt.detach().cpu().numpy().flatten().item()
        else:
            return vt.detach().cpu().numpy()
    elif isinstance(vt, np.ndarray):
        return vt
    else:
        raise TypeError('Input should be a ndarray or tensor')


# makes the random numbers predictable
def set_seed(rand_seed):
    np.random.seed(rand_seed)
    random.seed(rand_seed)
    torch.backends.cudnn.enabled = True
    torch.manual_seed(rand_seed)
    torch.cuda.manual_seed(rand_seed)

def select_gpus(gpus):
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    if gpus != '':
        print("Using gpu's: {}".format(gpus))
        os.environ["CUDA_VISIBLE_DEVICES"] = gpus
    else:
        print('Using default gpu.')

def may_mkdirs(dir_name):
    # if not os.cam_path.exists(os.cam_path.dirname(os.cam_path.abspath(fname))):
    #     os.makedirs(os.cam_path.dirname(os.cam_path.abspath(fname)))
    if not os.path.exists(os.path.abspath(dir_name)):
        os.makedirs(os.path.abspath(dir_name))


class AverageMeter(object):
    """ 
    Computes and stores the average and current value

    """

    def __init__(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / (self.count + 1e-20)


class RunningAverageMeter(object):
    """
    Computes and stores the running average and current value
    """

    def __init__(self, hist=0.99):
        self.val = None
        self.avg = None
        self.hist = hist

    def reset(self):
        self.val = None
        self.avg = None

    def update(self, val):
        if self.avg is None:
            self.avg = val
        else:
            self.avg = self.avg * self.hist + val * (1 - self.hist)
        self.val = val


class RecentAverageMeter(object):
    """
    Stores and computes the average of recent values
    """

    def __init__(self, hist_size=100):
        self.hist_size = hist_size
        self.fifo = []
        self.val = 0

    def reset(self):
        self.fifo = []
        self.val = 0

    def update(self, value):
        self.val = value
        self.fifo.append(value)
        if len(self.fifo) > self.hist_size:
            del self.fifo[0]

    @property
    def avg(self):
        assert len(self.fifo) > 0
        return float(sum(self.fifo)) / len(self.fifo)


class ReDirectSTD(object):
    """
    overwrites the sys.stdout or sys.stderr
    Args:
      fpath: file cam_path
      console: one of ['stdout', 'stderr']
      immediately_visiable: False
    Usage example:
      ReDirectSTD('stdout.txt', 'stdout', False)
      ReDirectSTD('stderr.txt', 'stderr', False)
    """

    def __init__(self, fpath=None, console='stdout', immediately_visiable=False):
        import sys
        import os
        assert console in ['stdout', 'stderr']
        self.console = sys.stdout if console == "stdout" else sys.stderr
        self.file = fpath
        self.f = None
        self.immediately_visiable = immediately_visiable

        if fpath is not None:
            # Remove existing log file
            if os.path.exists(fpath):
                os.remove(fpath)
        if console == 'stdout':
            sys.stdout = self
        else:
            sys.stderr = self

    def __del__(self):
        self.close()

    def __enter__(self):
        pass

    def __exit__(self, **args):
        self.close()

    def write(self, msg):
        self.console.write(msg)
        if self.file is not None:
            if not os.path.exists(os.path.dirname(os.path.abspath(self.file))):
                os.mkdir(os.path.dirname(os.path.abspath(self.file)))

            if self.immediately_visiable:
                # open for writing, appending to the end of the file if it exists
                with open(self.file, 'a') as f:
                    f.write(msg)
            else:
                if self.f is None:
                    self.f = open(self.file, 'w')

                # print("self.f is not none")
                # first time self.f is None, second is not None
                self.f.write(msg)

    def flush(self):
        self.console.flush()
        if self.f is not None:
            self.f.flush()
            import os
            os.fsync(self.f.fileno())

    def close(self):
        self.console.close()
        if self.f is not None:
            self.f.close()


def find_index(seq, item):
    for i, x in enumerate(seq):
        if item == x:
            return i
    return -1


def set_devices(sys_device_ids):
    """
    Args:
        sys_device_ids: a tuple; which GPUs to use
          e.g.  sys_device_ids = (), only use cpu
                sys_device_ids = (3,), use the 4-th gpu
                sys_device_ids = (0, 1, 2, 3,), use the first 4 gpus
                sys_device_ids = (0, 2, 4,), use the 1, 3 and 5 gpus
    """
    import os
    visiable_devices = ''
    for i in sys_device_ids:
        visiable_devices += '{}, '.format(i)
    os.environ['CUDA_VISIBLE_DEVICES'] = visiable_devices
    # Return wrappers
    # Models and user defined Variables/Tensors would be transferred to 
    # the first device
    device_id = 0 if len(sys_device_ids) > 0 else -1


def transfer_optims(optims, device_id=-1):
    for optim in optims:
        if isinstance(optim, torch.optim.Optimizer):
            transfer_optim_state(optim.state, device_id=device_id)


def transfer_optim_state(state, device_id=-1):
    """
    Transfer an optimizer.state to cpu or specified gpu, which means
    transferring tensors of the optimizer.state to specified device.
    The modification is in place for the state.
    Args:
        state: An torch.optim.Optimizer.state
        device_id: gpu id, or -1 which means transferring to cpu
    """
    for key, val in state.items():
        if isinstance(val, dict):
            transfer_optim_state(val, device_id=device_id)
        elif isinstance(val, Variable):
            raise RuntimeError("Oops, state[{}] is a Variable!".format(key))
        elif isinstance(val, torch.nn.Parameter):
            raise RuntimeError("Oops, state[{}] is a Parameter!".format(key))
        else:
            try:
                if device_id == -1:
                    state[key] = val.cpu()
                else:
                    state[key] = val.cuda(device=device_id)
            except:
                pass


def load_state_dict(model, src_state_dict):
    """
    copy parameter from src_state_dict to models
    Arguments:
        model: A torch.nn.Module object
        src_state_dict: a dict containing parameters and persistent buffers
    """
    from torch.nn import Parameter
    dest_state_dict = model.state_dict()
    for name, param in src_state_dict.items():
        if name not in dest_state_dict:
            continue
        if isinstance(param, Parameter):
            param = param.data
        try:
            dest_state_dict[name].copy_(param)
        except Exception as msg:
            print("Warning: Error occurs when copying '{}': {}".format(name, str(msg)))

    src_missing = set(dest_state_dict.keys()) - set(src_state_dict.keys())
    if len(src_missing) > 0:
        print("Keys not found in source state_dict: ")
        for n in src_missing:
            print('\t', n)

    dest_missint = set(src_state_dict.keys()) - set(dest_state_dict.keys())
    if len(dest_missint):
        print("Keys not found in destination state_dict: ")
        for n in dest_missint:
            print('\t', n)


def load_ckpt(modules_optims, ckpt_file, load_to_cpu=True, verbose=True):
    """
    load state_dict of module & optimizer from file
    Args:
        modules_optims: A two-element list which contains module and optimizer
        ckpt_file: the check point file 
        load_to_cpu: Boolean, whether to preprocess tensors in models & optimizer to cpu type
    """
    map_location = (lambda storage, loc: storage) if load_to_cpu else None
    ckpt = torch.load(ckpt_file, map_location=map_location)
    for m, sd in zip(modules_optims, ckpt['state_dicts']):
        m.load_state_dict(sd)
    if verbose:
        print("Resume from ckpt {}, \nepoch: {}, scores: {}".format(
            ckpt_file, ckpt['ep'], ckpt['scores']))
    return ckpt['ep'], ckpt['scores']


def save_ckpt(model, ckpt_files, epoch, metric):
    """
    Note:
        torch.save() reserves device type and id of tensors to save.
        So when loading ckpt, you have to inform torch.load() to load these tensors
        to cpu or your desired gpu, if you change devices.
    """

    if not os.path.exists(os.path.dirname(os.path.abspath(ckpt_files))):
        os.makedirs(os.path.dirname(os.path.abspath(ckpt_files)))

    save_dict = {'state_dicts': model.state_dict(),
                 'epoch': f'{time_str()} in epoch {epoch}',
                 'metric': metric}
    torch.save(save_dict, ckpt_files)


def adjust_lr_staircase(param_groups, base_lrs, ep, decay_at_epochs, factor):
    """ Multiplied by a factor at the beging of specified epochs. Different
        params groups specify thier own base learning rates.
    Args:
        param_groups: a list of params
        base_lrs: starting learning rate, len(base_lrs) = len(params_groups)
        ep: current epoch, ep >= 1
        decay_at_epochs: a list or tuple; learning rates are multiplied by a factor 
          at the begining of these epochs
        factor: a number in range (0, 1)
    Example:
        base_lrs = [0.1, 0.01]
        decay_at_epochs = [51, 101]
        factor = 0.1
    Note:
        It is meant to be called at the begining of an epoch
    """
    assert len(base_lrs) == len(param_groups), \
        'You should specify base lr for each param group.'
    assert ep >= 1, "Current epoch number should be >= 1"

    if ep not in decay_at_epochs:
        return

    ind = find_index(decay_at_epochs, ep)
    for i, (g, base_lr) in enumerate(zip(param_groups, base_lrs)):
        g['lr'] = base_lr * factor ** (ind + 1)
        print('=====> Param group {}: lr adjusted to {:.10f}'.format(i, g['lr']).rstrip('0'))


def may_set_mode(maybe_modules, mode):
    """
    maybe_modules, an object or a list of objects.
    """
    assert mode in ['train', 'eval']
    if not is_iterable(maybe_modules):
        maybe_modules = [maybe_modules]
    for m in maybe_modules:
        if isinstance(m, torch.nn.Module):
            if mode == 'train':
                m.train()
            else:
                m.eval()


def get_topk(matrix, k):
    """
    retain topk elements of a matrix and set others 0
    Args:
        matrix (object): np.array 2d
    """
    vector = matrix.reshape(matrix.shape[0] * matrix.shape[1])
    vector.sort()
    vector.get

    return matrix


class Timer:

    def __init__(self):
        self.o = time.time()

    def measure(self, p=1):
        x = (time.time() - self.o) / p
        x = int(x)
        if x >= 3600:
            return '{:.1f}h'.format(x / 3600)
        if x >= 60:
            return '{}m'.format(round(x / 60))
        return '{}s'.format(x)


class data_prefetcher():
    def __init__(self, loader):
        self.loader = iter(loader)
        self.stream = torch.cuda.Stream()
        # self.mean = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]).cuda().view(1, 3, 1, 1)
        # self.std = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]).cuda().view(1, 3, 1, 1)
        # With Amp, it isn't necessary to manually convert data to half.
        # if args.fp16:
        #     self.mean = self.mean.half()
        #     self.std = self.std.half()
        self.preload()

    def preload(self):
        try:
            self.next_input, self.next_target = next(self.loader)
        except StopIteration:
            self.next_input = None
            self.next_target = None
            return
        with torch.cuda.stream(self.stream):
            self.next_input = self.next_input.cuda(non_blocking=True)
            self.next_target = self.next_target.cuda(non_blocking=True)
            # With Amp, it isn't necessary to manually convert data to half.
            # if args.fp16:
            #     self.next_input = self.next_input.half()
            # else:
            self.next_input = self.next_input.float()
            # self.next_input = self.next_input.sub_(self.mean).div_(self.std)

    def next(self):
        torch.cuda.current_stream().wait_stream(self.stream)
        input = self.next_input
        target = self.next_target
        self.preload()
        return input, target

文件结束
新文件：tools\__init__.py

文件开始

文件结束
新文件：utils\distributed.py

文件开始
#!/usr/bin/env python3

"""Distributed helpers."""

import torch
import torch.distributed as dist
_LOCAL_PROCESS_GROUP = None


def get_world_size() -> int:
    if not dist.is_available():
        return 1
    if not dist.is_initialized():
        return 1
    return dist.get_world_size()


def get_rank() -> int:
    if not dist.is_available():
        return 0
    if not dist.is_initialized():
        return 0
    return dist.get_rank()


def is_master_process(num_gpus=8):
    """
    Determines if the current process is the master process.
    """
    if torch.distributed.is_initialized():
        return dist.get_rank() % num_gpus == 0
    else:
        return True


def run(
    local_rank,
    num_proc,
    func,
    init_method,
    shard_id,
    num_shards,
    backend,
    cfg,
    args,
):
    """
    Runs a function from a child process.
    Args:
        local_rank (int): rank of the current process on the current machine.
        num_proc (int): number of processes per machine.
        func (function): function to execute on each of the process.
        init_method (string): method to initialize the distributed training.
            TCP initialization: equiring a network address reachable from all
            processes followed by the port.
            Shared file-system initialization: makes use of a file system that
            is shared and visible from all machines. The URL should start with
            file:// and contain a path to a non-existent file on a shared file
            system.
        shard_id (int): the rank of the current machine.
        num_shards (int): number of overall machines for the distributed
            training job.
        backend (string): three distributed backends ('nccl', 'gloo', 'mpi') are
            supports, each with different capabilities. Details can be found
            here:
            https://pytorch.org/docs/stable/distributed.html
        cfg (CfgNode): configs. Details can be found in
            loco/config/defaults.py
    """
    # Initialize the process group.
    # shard_id = get_rank()
    world_size = num_proc * num_shards
    rank = shard_id * num_proc + local_rank

    try:
        torch.distributed.init_process_group(
            backend=backend,
            init_method=init_method,
            world_size=world_size,
            rank=rank,
        )
    except Exception as e:
        raise e

    torch.cuda.set_device(local_rank)
    func(cfg, args)


def destroy_process_group():
    """Destroys the default process group."""
    torch.distributed.destroy_process_group()


def scaled_all_reduce(cfg, tensors):
    """Performs the scaled all_reduce operation on the provided tensors.

    The input tensors are modified in-place. Currently supports only the sum
    reduction operator. The reduced values are scaled by the inverse size of
    the process group (equivalent to cfg.NUM_GPUS).
    """
    # Queue the reductions
    reductions = []
    for tensor in tensors:
        reduction = torch.distributed.all_reduce(tensor, async_op=True)
        reductions.append(reduction)
    # Wait for reductions to finish
    for reduction in reductions:
        reduction.wait()
    # Scale the results
    for tensor in tensors:
        tensor.mul_(1.0 / cfg.NUM_GPUS / cfg.NUM_SHARDS)
    return tensors


def cat_all_gather(tensors):
    """Performs the concatenated all_gather operation on the provided tensors.
    """
    tensors_gather = [
        torch.ones_like(tensors)
        for _ in range(torch.distributed.get_world_size())
    ]
    torch.distributed.all_gather(tensors_gather, tensors, async_op=False)

    output = torch.cat(tensors_gather, dim=0)
    return output


def local_cat_all_gather(tensors):
    """Performs the concatenated all_gather operation on the provided tensors.
    """
    tensors_gather = [
        torch.ones_like(tensors)
        for _ in range(get_local_size())
    ]
    torch.distributed.all_gather(
        tensors_gather,
        tensors,
        async_op=False,
        group=_LOCAL_PROCESS_GROUP,
    )
    output = torch.cat(tensors_gather, dim=0)
    return output


def get_local_size():
    """
    Returns:
        The size of the per-machine process group,
        i.e. the number of processes per machine.
    """
    if not dist.is_available():
        return 1
    if not dist.is_initialized():
        return 1
    return dist.get_world_size(group=_LOCAL_PROCESS_GROUP)


def get_local_rank():
    """
    Returns:
        The rank of the current process within the local (per-machine) process group.
    """
    if not dist.is_available():
        return 0
    if not dist.is_initialized():
        return 0
    assert _LOCAL_PROCESS_GROUP is not None
    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)

文件结束
新文件：utils\file_io.py

文件开始
#!/usr/bin/env python3

"""
Project specific pathmanagers for a project as recommended by Detectron2
"""
from iopath.common.file_io import PathManager as PathManagerBase
from iopath.common.file_io import HTTPURLHandler


PathManager = PathManagerBase()
PathManager.register_handler(HTTPURLHandler())

文件结束
新文件：utils\io_utils.py

文件开始
#!/usr/bin/env python3
"""
a bunch of helper functions for read and write data
"""
import os
import json
import numpy as np
import time
import pandas as pd

from typing import List, Union
from PIL import Image, ImageFile
Image.MAX_IMAGE_PIXELS = None


def save_or_append_df(out_path, df):
    if os.path.exists(out_path):
        previous_df = pd.read_pickle(out_path)
        df = pd.concat([previous_df, df], ignore_index=True)
    df.to_pickle(out_path)
    print(f"Saved output at {out_path}")


class JSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, bytes):
            return str(obj, encoding='utf-8')
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            # return super(MyEncoder, self).default(obj)

            raise TypeError(
                "Unserializable object {} of type {}".format(obj, type(obj))
            )


def write_json(data: Union[list, dict], outfile: str) -> None:
    json_dir, _ = os.path.split(outfile)
    if json_dir and not os.path.exists(json_dir):
        os.makedirs(json_dir)

    with open(outfile, 'w') as f:
        json.dump(data, f, cls=JSONEncoder, ensure_ascii=False, indent=2)


def read_json(filename: str) -> Union[list, dict]:
    """read json files"""
    with open(filename, "rb") as fin:
        data = json.load(fin)
    return data


def pil_loader(path: str) -> Image.Image:
    """load an image from path, and suppress warning"""
    # to avoid crashing for truncated (corrupted images)
    ImageFile.LOAD_TRUNCATED_IMAGES = True
    # open path as file to avoid ResourceWarning
    # (https://github.com/python-pillow/Pillow/issues/835)
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')

文件结束
新文件：utils\logging.py

文件开始
#!/usr/bin/env python3

"""Logging."""

import builtins
import decimal
import functools
import logging
import simplejson
import sys
import os
from termcolor import colored

from .distributed import is_master_process
from .file_io import PathManager

# Show filename and line number in logs
_FORMAT = "[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s"


def _suppress_print():
    """Suppresses printing from the current process."""

    def print_pass(*objects, sep=" ", end="\n", file=sys.stdout, flush=False):
        pass

    builtins.print = print_pass


# cache the opened file object, so that different calls to `setup_logger`
# with the same file name can safely write to the same file.
@functools.lru_cache(maxsize=None)
def _cached_log_stream(filename):
    return PathManager.open(filename, "a")


@functools.lru_cache()  # so that calling setup_logger multiple times won't add many handlers  # noqa
def setup_logging(
    num_gpu, num_shards, output="", name="visual_prompt", color=True):
    """Sets up the logging."""
    # Enable logging only for the master process
    if is_master_process(num_gpu):
        # Clear the root logger to prevent any existing logging config
        # (e.g. set by another module) from messing with our setup
        logging.root.handlers = []
        # Configure logging
        logging.basicConfig(
            level=logging.INFO, format=_FORMAT, stream=sys.stdout
        )
    else:
        _suppress_print()

    if name is None:
        name = __name__
    logger = logging.getLogger(name)
    # remove any lingering handler
    logger.handlers.clear()

    logger.setLevel(logging.INFO)
    logger.propagate = False

    plain_formatter = logging.Formatter(
        "[%(asctime)s][%(levelname)s] %(name)s: %(lineno)4d: %(message)s",
        datefmt="%m/%d %H:%M:%S",
    )
    if color:
        formatter = _ColorfulFormatter(
            colored("[%(asctime)s %(name)s]: ", "green") + "%(message)s",
            datefmt="%m/%d %H:%M:%S",
            root_name=name,
            abbrev_name=str(name),
        )
    else:
        formatter = plain_formatter

    if is_master_process(num_gpu):
        ch = logging.StreamHandler(stream=sys.stdout)
        ch.setLevel(logging.DEBUG)
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    if is_master_process(num_gpu * num_shards):
        if len(output) > 0:
            if output.endswith(".txt") or output.endswith(".log"):
                filename = output
            else:
                filename = os.path.join(output, "logs.txt")

            PathManager.mkdirs(os.path.dirname(filename))

            fh = logging.StreamHandler(_cached_log_stream(filename))
            fh.setLevel(logging.DEBUG)
            fh.setFormatter(plain_formatter)
            logger.addHandler(fh)
    return logger


def setup_single_logging(name, output=""):
    """Sets up the logging."""
    # Enable logging only for the master process
    # Clear the root logger to prevent any existing logging config
    # (e.g. set by another module) from messing with our setup
    logging.root.handlers = []
    # Configure logging
    logging.basicConfig(
        level=logging.INFO, format=_FORMAT, stream=sys.stdout
    )

    if len(name) == 0:
        name = __name__
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    logger.propagate = False

    plain_formatter = logging.Formatter(
        "[%(asctime)s][%(levelname)s] %(name)s: %(lineno)4d: %(message)s",
        datefmt="%m/%d %H:%M:%S",
    )
    formatter = _ColorfulFormatter(
        colored("[%(asctime)s %(name)s]: ", "green") + "%(message)s",
        datefmt="%m/%d %H:%M:%S",
        root_name=name,
        abbrev_name=str(name),
    )

    ch = logging.StreamHandler(stream=sys.stdout)
    ch.setLevel(logging.DEBUG)
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    if len(output) > 0:
        if output.endswith(".txt") or output.endswith(".log"):
            filename = output
        else:
            filename = os.path.join(output, "logs.txt")

        PathManager.mkdirs(os.path.dirname(filename))

        fh = logging.StreamHandler(_cached_log_stream(filename))
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(plain_formatter)
        logger.addHandler(fh)

    return logger


def get_logger(name):
    """Retrieves the logger."""
    return logging.getLogger(name)


def log_json_stats(stats, sort_keys=True):
    """Logs json stats."""
    # It seems that in Python >= 3.6 json.encoder.FLOAT_REPR has no effect
    # Use decimal+string as a workaround for having fixed length values in logs
    logger = get_logger(__name__)
    stats = {
        k: decimal.Decimal("{:.6f}".format(v)) if isinstance(v, float) else v
        for k, v in stats.items()
    }
    json_stats = simplejson.dumps(stats, sort_keys=True, use_decimal=True)
    if stats["_type"] == "test_epoch" or stats["_type"] == "train_epoch":
        logger.info("json_stats: {:s}".format(json_stats))
    else:
        logger.info("{:s}".format(json_stats))


class _ColorfulFormatter(logging.Formatter):
    # from detectron2
    def __init__(self, *args, **kwargs):
        self._root_name = kwargs.pop("root_name") + "."
        self._abbrev_name = kwargs.pop("abbrev_name", "")
        if len(self._abbrev_name):
            self._abbrev_name = self._abbrev_name + "."
        super(_ColorfulFormatter, self).__init__(*args, **kwargs)

    def formatMessage(self, record: logging.LogRecord) -> str:
        record.name = record.name.replace(self._root_name, self._abbrev_name)
        log = super(_ColorfulFormatter, self).formatMessage(record)
        if record.levelno == logging.WARNING:
            prefix = colored("WARNING", "red", attrs=["blink"])
        elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:
            prefix = colored("ERROR", "red", attrs=["blink", "underline"])
        else:
            return log
        return prefix + " " + log

文件结束
新文件：utils\train_utils.py

文件开始
#!/usr/bin/env python3
import torch


def gpu_mem_usage():
    """Computes the GPU memory usage for the current device (GB)."""
    if not torch.cuda.is_available():
        return 0
    # Number of bytes in a megabyte
    _B_IN_GB = 1024 * 1024 * 1024

    mem_usage_bytes = torch.cuda.max_memory_allocated()
    return mem_usage_bytes / _B_IN_GB


class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self, name, fmt=':f'):
        self.name = name
        self.fmt = fmt
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

    def __str__(self):
        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
        return fmtstr.format(**self.__dict__)

文件结束
新文件：utils\vis_utils.py

文件开始
import datetime
import os
import glob
import numpy as np
import pandas as pd
import torch

from tqdm import tqdm
from collections import defaultdict
from sklearn.metrics import confusion_matrix
# plt.rcParams["axes.grid"] = False

import warnings
warnings.filterwarnings("ignore")
LOG_NAME = "logs.txt"


def remove_trailing(eval_dict):
    min_num = min([len(v) for k, v in eval_dict.items() if "top5" not in k])
    new_dict ={}
    for k, v in eval_dict.items():
        if "top5" not in k:
            new_dict[k] = v[:min_num]
    return new_dict


def get_meta(job_root, job_path, model_type):
    # get lr, wd, feature-type, dataset
    j_data = job_path.split("/run")[0].split(
        job_root + "/" + model_type)[-1].split("/")
    data_name, feat_type, opt_params = j_data[1], j_data[2], j_data[3]
    lr = float(opt_params.split("_")[0].split("lr")[-1])
    wd = float(opt_params.split("_")[1].split("wd")[-1])
    return data_name, feat_type, lr, wd


def update_eval(line, eval_dict, data_name):        
    if "top1" in line and "top" in line.split(": top1:")[-1]:
        metric = "top"     
    else:
        metric = "rocauc"
    top1 = float(line.split(": top1:")[-1].split(metric)[0])
    eval_type = line.split(" Classification results with ")[-1].split(": top1")[0] 
    eval_type = "".join(eval_type.split("_" + data_name))
    eval_dict[eval_type + "_top1"].append(top1)


def get_nmi(job_path):
    with open(job_path) as f:
        lines = f.readlines()
    nmi_dict = defaultdict(list)
    num_jobs = 0
    log_temp = []
    for l in lines:  #, leave=False):
        if "Rank of current process:" in l:
            num_jobs += 1
        if num_jobs == 2:
            break
        if "Clutering nmi" in l:
            n = l.split("Clutering nmi: ")[-1].split(",")[0]
            a_n = l.split("adjusted nmi: ")[-1].split(",")[0]
            v = l.split("v: ")[-1].split(",")[0]
            nmi_dict["nmi"].append(float(n))
            nmi_dict["a_nmi"].append(float(a_n))
            nmi_dict["v_nmi"].append(float(v))
    return nmi_dict


def get_mean_accuracy(job_path, data_name):
    val_data = torch.load(
        job_path.replace("logs.txt", f"val_{data_name}_logits.pth"))
    test_data = torch.load(
        job_path.replace("logs.txt", f"val_{data_name}_logits.pth"))
    v_matrix = confusion_matrix(
        val_data['targets'],
        np.argmax(val_data['joint_logits'], 1)
    )
    t_matrix = confusion_matrix(
        test_data['targets'],
        np.argmax(test_data['joint_logits'], 1)
    )
    return np.mean(v_matrix.diagonal()/v_matrix.sum(axis=1) ) * 100, np.mean(t_matrix.diagonal()/t_matrix.sum(axis=1) ) * 100


def get_training_data(job_path, model_type, job_root):
    data_name, feat_type, lr, wd = get_meta(job_root, job_path, model_type)
    with open(job_path) as f:
        lines = f.readlines()

    # get training loss per epoch, 
    # cls results for both val and test
    train_loss = []
    eval_dict = defaultdict(list)
#     best_epoch = -1
    num_jobs = 0
    total_params = -1
    gradiented_params = -1
    batch_size = None
    for line in lines:  #, leave=False):
        if "{'BATCH_SIZE'" in line and batch_size is None:
            batch_size = int(line.split("'BATCH_SIZE': ")[-1].split(",")[0])
            
        if "Total Parameters: " in line:
            total_params = int(line.split("Total Parameters: ")[-1].split("\t")[0])
            gradiented_params = int(line.split("Gradient Parameters: ")[-1].split("\n")[0])

        if "Rank of current process:" in line:
            num_jobs += 1
        if num_jobs == 2:
            break
        if "average train loss:" in line:
            loss = float(line.split("average train loss: ")[-1])
            train_loss.append(loss)
        if " Classification results with " in line:
            update_eval(line, eval_dict, data_name)

    meta_dict = {
        "data": data_name,
        "feature": feat_type,
        "lr": float(lr) * 256 / int(batch_size),
        "wd": wd,
        "total_params": total_params,
        "tuned_params": gradiented_params,
        "tuned / total (%)": round(gradiented_params / total_params * 100, 4),
        "batch_size": batch_size,
    }
    v_top1, t_top1 = None, None
    return train_loss, eval_dict, meta_dict, (v_top1, t_top1)


def get_time(file):
    with open(file) as f:
        lines = f.readlines()
    start_time = lines[0].split("[")[1].split("]")[0]
    start_time = datetime.datetime.strptime(start_time, '%m/%d %H:%M:%S')

    end_time = lines[-1].split("[")[1].split("]")[0]
    end_time = datetime.datetime.strptime(end_time, '%m/%d %H:%M:%S')

    per_iter = None
    with open(file) as f:
        lines = f.readlines()

    per_batch = []
    per_batch_train = []
    for line in lines[::-1]:
#         print(line)"Test 6/6. loss: 6.097, "
        if ". loss:" in line and "Test" in line:
            per_iter = line.split(" s / batch")[0].split(",")[-1]
            per_batch.append(float(per_iter))
        if ". train loss:" in line:
            per_iter = line.split(" s / batch")[0].split(",")[-1]
            per_batch_train.append(float(per_iter))
            
    return datetime.timedelta(seconds=(end_time-start_time).total_seconds()), np.mean(per_batch), np.mean(per_batch_train)


def get_df(files, model_type, root, is_best=True, is_last=True, max_epoch=300):
    pd_dict = defaultdict(list)
    for job_path in tqdm(files, desc=model_type):
        train_loss, eval_results, meta_dict, (v_top1, t_top1) = get_training_data(job_path, model_type, root)
        batch_size = meta_dict["batch_size"]
        
        if len(eval_results) == 0:
            print(f"job {job_path} not ready")
            continue
        if len(eval_results["val_top1"]) == 0:
            print(f"job {job_path} not ready")
            continue

        if "val_top1" not in eval_results or "test_top1" not in eval_results:
            print(f"inbalanced: {job_path}")
            continue
                
        for k, v in meta_dict.items():
            pd_dict[k].append(v)
        
        metric_b = "val_top1"
        best_epoch = np.argmax(eval_results[metric_b])

        if is_best:
            for name, val in eval_results.items():
                if "top5" in name:
                    continue
                if len(val) == 0:
                    continue
                if not isinstance(val[0], list):
                    try:
                        pd_dict["b-" + name].append(val[best_epoch])
                    except:
                        pd_dict["b-" + name].append(-1)
                        # ongoing training process
                        print(name, best_epoch, val)
        # last epoch
        if is_last:
            if v_top1 is not None:
                pd_dict["l-val_top1"].append(v_top1)
                pd_dict["l-test_top1"].append(t_top1)
                val = eval_results["val_top1"]
            else:
                for name, val in eval_results.items():
                    if "top5" in name:
                        continue
                    if len(val) == 0:
                        continue
                    pd_dict["l-" + name].append(val[-1])

        pd_dict["best_epoch"].append(f"{best_epoch + 1} | {len(val)}")

        pd_dict["file"].append(job_path)
        total_time, _, _ = get_time(job_path)
        pd_dict["total_time"].append(total_time)

    result_df = None
    if len(pd_dict) > 0:
        result_df = pd.DataFrame(pd_dict)
        result_df = result_df.sort_values(['data', "feature", "lr", "wd"])
    return result_df


def delete_ckpts(f):
    # delete saved ckpts for re
    f_dir, _ = os.path.split(f)
    for f_delete in glob.glob(f_dir + "/*.pth"):
        os.remove(f_delete)
        print(f"removed {f_delete}")


def average_df(df, metric_names=["l-val_top1", "l-val_base_top1"], take_average=True):
    # for each data and features and train type, display the averaged results
    data_names = set(list(df["data"]))
    f_names = set(list(df["feature"]))
    t_names = set(list(df["type"]))
    hp_names = [
        c for c in df.columns if c not in ["data", "feature", "type", "file", "best_epoch"] + metric_names]
    data_dict = defaultdict(list)
    for d_name in data_names:
        for f_name in f_names:
            for t_name in t_names:

                result = df[df.data == d_name]
                result = result[result.feature == f_name]
                result = result[result.type == t_name]
                # take average here
                if len(result) == 0:
                    continue
                data_dict["data"].append(d_name)
                data_dict["feature"].append(f_name)
                data_dict["type"].append(t_name)
                data_dict["total_runs"].append(len(result))
        
                for m in metric_names:
                    if take_average:
                        data_dict[m].append("{:.2f}".format(
                            np.mean([r for i, r in enumerate(result[m])]),
                        ))
                        data_dict[f"{m}-std"].append("{:.2f}".format(
                            np.std([r for i, r in enumerate(result[m])])
                        ))
                    else:
                        data_dict[m].append("{:.2f}".format(
                            np.median([r for i, r in enumerate(result[m])]),
                        ))
                for h_name in hp_names:
                    data_dict[h_name].append(result[h_name].iloc[0])

    df = pd.DataFrame(data_dict)
    df = df.sort_values(["data", "feature", "type"])
    return df


def filter_df(df, sorted_cols, max_num):
    # for each data and features, display only top max_num runs
    data_names = set(list(df["data"]))
    f_names = set(list(df["feature"]))
    t_names = set(list(df["type"]))
    df_list = []
    for d_name in data_names:
        for f_name in f_names:
            for t_name in t_names:
                result = df[df.data == d_name]
                result = result[result.feature == f_name]
                result = result[result.type == t_name]
                if len(result) == 0:
                    continue
                cols = [c for c in sorted_cols if c in result.columns]
                result = result.sort_values(cols, ignore_index=True)

                _num = min([max_num, len(result)])
    #             print(result.iloc[-_num:])
                df_list.append(result.iloc[-_num:])
    return pd.concat(df_list)


def display_results(df, sorted_cols=["data", "feature", "type", "l-val_top1"], max_num=1):
    cols = [c for c in df.columns if c not in []]
    df = df[cols]
    if max_num is not None:
        df = filter_df(df, sorted_cols[3:], max_num)
    return df.sort_values(sorted_cols).reset_index(drop=True)

文件结束
